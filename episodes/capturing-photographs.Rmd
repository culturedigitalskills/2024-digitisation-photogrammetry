---
title: 'Capturing images'
teaching: 10
exercises: 2
--- 

:::::::::::::::::::::::::::::::::::::: questions
- What are the premises before a shooting session?

- How do we capture images ?

::::::::::::::::::::::::::::::::::::::::::::::::


::::::::::::::::::::::::::::::::::::: objectives

- Show how to setup a camera.

- Shows various scenarios of image capturing and shooting environments.

::::::::::::::::::::::::::::::::::::::::::::::::






## Camera setup

Please [refer to the equipment][setup] that you will need
access to.

You will need to consider the following
camera settings during your acquisition
of photographs:


-	Preferably shoot in **RAW** and in maximal resolution. JPG compression creates noise that should be avoided. If JPG images are to be used, then prefer high quality JPG images.
-	**ISO** values should be the **lowest possible** as you want **clear, sharp images** without too much noise. ISO 100 will provide good pictures without much noise <!--but for this you will need a tripod because longer shutter speeds will be required. For hand-held camera you can go up to ISO 800 but this will bring more grain to your pictures.-->
-	**Aperture** value (f number) should be **high enough** so as to be able to distinguish details without having blurred surfaces. A higher f number means that you will get a **better depth of field**. Something between f/8 and f/16 would work well.
-	**Shutter speed** should be **fast enough** to freeze images and avoid blur that is caused by the movement of the camera. If you are using a tripod you can use slower shutter speeds. The rule here is that anything below **(slower) than 1/60 of a second requires a tripod**.
- You should consider always a **large depth of field** when possible as **good focus** especially on the subject is important. Be careful to have all the important parts of the image in focus. Automatic focus is not advice because you would not want to change focus for each picture you take. Once you have your subject in focus keep it as it is and try not to change your distance from the subject. For a better explanation on how **depth of field** works in conjunction with **aperture, focal length and focus distance** you can refer to the website [Cambridge in Color](https://www.cambridgeincolour.com/tutorials/depth-of-field.htm) or [Photographylife](https://photographylife.com/what-is-depth-of-field)<!--can be used when you are rotating around the object, but you can set focus manually if you are using a turntable-->.


|   |   |
|---|---|
| ![Depth-of-field](https://upload.wikimedia.org/wikipedia/commons/6/6f/Depth-of-field.svg) | <iframe width="560" height="315" src="https://www.youtube.com/embed/oXOHRkMHDC8?si=Ci1AADklFc2Eidjo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe> |
| (a) top: large depth of field (b) bottom: small depth of field (2) depth of field (1) plane of focus, Public domain, under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0), via [Wikimedia Commons](https://upload.wikimedia.org/wikipedia/commons/6/6f/Depth-of-field.svg) | How to Use Depth of Field in Photography - Explained, Photography Life, under [Photography Life](https://photographylife.com/), via [YouTube](https://www.youtube.com/watch?v=oXOHRkMHDC8) |



\

> **Example of settings:** f/8, ISO 400, shutter speed 1/30 and if light is not enough you can increase ISO up to 800 Or better to lower the shutter speed to 1/15 (remember that any shutter speed that is lower than 1/60 requires a tripod). Please note that these are just examples and you should check exposure for every acquisition depending on current light conditions.


## Capturing images
The process for capturing photograph in photogrammetry differs significantly from classical photography. Basic rules must be followed in order to achieve good results.
Start the acquisition from an angle/view of the object that has many details and is not very plain.
[**Here**](https://www.3dflow.net/technology/documents/photogrammetry-how-to-acquire-pictures/) you can find an introduction for best practice when shoothing for photogrammetry. This is from the makers of the software that we are going to use later in the processing stage.

### Overlapping
You need enough image overlap, around 50-60%, to make sure that the software will be able to align the images correctly.

Remember that you should avoid having *blind-zones* and the object should occupy the maximum possible frame area. 

Close-up photos are allowed to capture minor details.

|   |
|---|
| ![Ground horizontal shooting](https://upload.wikimedia.org/wikipedia/commons/d/d7/%EC%A7%80%EC%83%81%EC%88%98%ED%8F%89%EC%B4%AC%EC%98%81.png){width="100%"} | |
| Ground horizontal shooting, Gcd822C under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0), via [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:%EC%A7%80%EC%83%81%EC%88%98%ED%8F%89%EC%B4%AC%EC%98%81.png) |


### Number of images
20-60 for each 360 acquisition. Remember that it is better to have more images 
than less. *Bad* images (e.g. blurred, not in focus) can be deleted before processing.

|   |
|---|
| ![Statue of Queen Victoria Brighton](https://data.d4science.org/shub/E_UTNCcDhmdUlGV21qZmdQazROUXpBTXordC9ndTV3cnJHTG9QWkJDTm5wZFVnbFlTYUJiSFhqQklGNEU0KzZySQ==){width="100%"} | |
| Statue of Queen Victoria Brighton, DSVMC University of Brighton, under [ DSVMC](https://culturedigitalskills.org/), via [D4Science](https://www.d4science.org/) |

### Texture
Plain and monotonous surfaces should be avoided. 

Flat, very thin artefacts and textures such as fur, hair won’t be the ideal candidates for photogrammetry. 

Crossing objects (e.g. leaves of a tree) and moving objects are not good candidates either. Significant colour changes or colour designs on a relatively plain surface could provide good reference points and help us to produce a model. The best candidates are solid, matte, textured artefacts.

|   |   |
|---|---|
| ![LN plain ware jug - Herakleion AM 02](https://upload.wikimedia.org/wikipedia/commons/6/6c/LN_plain_ware_jug_-_Herakleion_AM_02.jpg){width="90%"}  | ![Baekja-White Ceramic](https://upload.wikimedia.org/wikipedia/commons/4/4c/Baekja-White_Ceramic.jpg){width="100%"} |
| **Good candidate** object type / vase shape: Cretan neolithic jug with 2 rows of smaller and larger lug handles - material: pottery (clay) - decoration technique: undecorated, burnished - style / ware: plain ware - period: late neolithic (dating in the museum: 3600-3000) - findspot: Ida Cave or Amnisos?? - museum / inventory number: Heraklion, Archaeological Museum, Public domain, 	ArchaiOptix, under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0), via [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:LN_plain_ware_jug_-_Herakleion_AM_02.jpg) |**Bad candidate** Baekja or Korean White Ceramic (백자;白磁) in an exhibition titled “The Light of Millennium of Korean Ceramics” in Jakarta, Indonesia, November 2008, Public Domain, Taman Renyah, under [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0), via [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Baekja-White_Ceramic.jpg) | 

### Reflections and Transparency
Some objects are shiny and the reflections will result in having lots of noise, hence a ‘bad’ model. Transparency is also a problem because it is very hard to estimate the 3d position of a glass as the algorithm can get confused on the real position of the object.

Adding talc or corn-starch on the surface of the object could be a solution but this cannot be applied on most cultural heritage artefacts.

|   |
|---|
| ![Balsamario in vetro trasparente IEU 0370](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4b/Balsamario_in_vetro_trasparente_1_13S9902.tif/lossy-page1-1280px-Balsamario_in_vetro_trasparente_1_13S9902.tif.jpg){width="100%"} | |
| Transparent glass unguent bottle Glass, Egyptian Museum Turin (IT), under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0),  via [Wikimedia](https://commons.wikimedia.org/wiki/File:Balsamario_in_vetro_trasparente_IEU_0370.tif) |

### Targets/markers
You can put markers and targets on/around/underneath the object that you want to acquire to help the software with the aligning process. 

To support accurate measurements of 3D data you can also place a calibrated scale image underneath the object (or scale bars around it). 

Remember that these points should remain in the same position with respect to the object. So, if you move with the camera around the object they should remain in the same place (e.g. placed around the object) but if you are using a turntable they should turn along with the object (e.g. placed underneath the object).


|   |
|---|
| ![Wooden horse](https://data.d4science.org/shub/E_aEQwQ29YZEl4Qnc1aVE5Y0ovZHhmYWhvSHdFbTBaSGdSQ1BFUy9MUzZEbkNOUmdqKzZIMTh5ZjBaZlhjdEt0ag==){width="100%"} | |
|Wooden horse, DSVMC University of Brighton, under [ DSVMC](https://culturedigitalskills.org/), via [D4Science](https://www.d4science.org/) |

 

### Lighting
Good lighting is required and occlusions should be kept to minimum. The ideal conditions for an outdoor acquisition require an overcast/cloudy day. 

If there is sun that creates shadows, you can use a sheet to shade the object of interest when possible. 

For indoor acquisition, you can use static artificial light. In this case, lights should have the same intensity. It is better to use diffused light that is projected on every surface of the object equally. Two light sources can be placed on the sides of the object at an angle of 45 degrees and one can come from the top. Shadows should be avoided as much as possible (thus you might want to add more light sources, for example one at the back).

|   |   |
|---|---|
| ![Saint Viktor of Xanten Church](https://upload.wikimedia.org/wikipedia/commons/b/b7/D%C3%BClmen%2C_St.-Viktor-Kirche_--_2014_--_0076.jpg){width="90%"}  | ![Statue of Queen Victoria Brighton (UK)](https://data.d4science.org/shub/E_UGlQR3RLSzdPZm4xZ0ZINlVXOVR1Ukd4aTFNV1V6RWxRRU16aEE1dEV6TkhYZ0wyQTBuSW0zdFN6U1d4V1NEMg==){width="90%"} |
| **Bad candidate**  Saint Viktor of Xanten Church, Dülmen, North Rhine-Westphalia, Germany, Public domain, Dietmar Rabich , under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0), via [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:D%C3%BClmen,_St.-Viktor-Kirche_--_2014_--_0076.jpg) |**Good candidate** Statue of Queen Victoria Brighton, DSVMC University of Brighton, under [ DSVMC](https://culturedigitalskills.org/), via [D4Science](https://www.d4science.org/) |


### Background
This should be kept simple and plain. There should be high contrast between the object and the background (e.g. dark object requires bright background and viceversa).

|   |
|---|
| ![Balkan Heritage Field School-5](https://upload.wikimedia.org/wikipedia/commons/d/d3/Balkan_Heritage_Field_School-5.jpg){width="90%"}  |
|  Balkan Heritage Field School (photogrammetry course) at Stobi, Republic of Macedonia, Ivan.giogio, under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0), via [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Balkan_Heritage_Field_School-5.jpg) 


## Different types of acquisition scenarios (objects and evironments)

### Object on turntable and camera on tripod
The object is placed on a turntable and the camera on a tripod. 

If artificial light is used, this should be diffused and should not create shadows. 

The camera should be placed at a height that allows to see all important features of the 
artefact (e.g. at an angle of 45 degrees above the object). 

The advantage of this method is that you can have lower ISO and shutter speeds and thus sharper images (especially in indoor environments).

|   |   |
|---|---|
| <iframe width="560" height="315" src="https://www.youtube.com/embed/REA3XNgUMJg?si=mFPGNKpIaUf5crHb" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>  | ![Wooden Horse Turntable](https://data.d4science.org/shub/E_dUFhdW4vd2x0SVRNOWFjOWVjc1pwU2FyaFJXWlA2VnBzWjI1QnN5L3UvalMxYlVVSFhkMmEwb0FqTlkrdHZaMg==){width="100%"} |
|Photogrammetry Setup for Indoor 3D Prop Scanning,Grzegorz Baran, under [Grzegorz Baran](https://www.artstation.com/gbaran), via [YouTube](https://www.youtube.com/watch?v=REA3XNgUMJg) | Wooden Horse Turntable, DSVMC University of Brighton, under [ DSVMC](https://culturedigitalskills.org/), via [D4Science](https://www.d4science.org/) |

> Here you can also find a [**video**](https://www.youtube.com/watch?v=Fj7wGGXPM0A) of an interesting automatic DIY rig that will speed up the process when shooting small and medium objects by [**Openscan.eu**](https://en.openscan.eu/) 

\

### Object at the centre and camera moves around

The object is placed at the centre. You move around it and take pictures with the camera. 
Place the item at a good height so that it is possible to take images from a higher and a lower level. 

Start by taking an image every 10-15 degrees horizontally with 50-60% overlapping. 

As soon as you finish with one series of images around the object, raise (or lower) the camera 
10-15 degrees vertically and take another round of photos. 
In case some areas are not that visible, remember to take different pictures of that part from different angles. The advantage of this method is that it will allow you to acquire larger objects without setting up lights.

Here a **video** of an interesting DIY rig with camera moving around, maintaining the same distance from the object.

|   | 
|---|
| <iframe width="560" height="315" src="https://www.youtube.com/embed/28vrZIj-hYQ?si=ue8yPd0GA0-QMgv1&amp;start=487" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe> |
| Building a 3D Scanner Turntable: Advanced Photogrammetry Agisoft Metashape, Eric Strebel, under [Eric Strebel](https://www.botzen.com/), via [YouTube](https://www.youtube.com/watch?v=28vrZIj-hYQ&t=487s) |


\

### Environment of closed-space (rooms)


\



### Environment of open-space (facades)


\




## Tips for acquisition
Good acquisition of images is important in order to have a successful result. 
Please have in mind that the right/optimal setup will provide you with the correct dataset that will work properly with the software and will provide you with a good 3D model.

- Try overlapping of at least 60% of each image
- Avoid shooting agianst the sun when you are outside
- Capture images with good texture.
- Avoid completely texture-less, transparent and reflective images. The software will have difficulty finding and matching features. If the scene does not contain enough texture itself, you could place additional background objects, such as posters, etc.

\
\
<!--
---
title: 'Processing overview'
teaching: 10
exercises: 2
--- -->



:::::::::::::::::::::::::::::::::::::: questions 

- How do you create 3d models for 3D digital preservation and publication ?

::::::::::::::::::::::::::::::::::::::::::::::::


::::::::::::::::::::::::::::::::::::: objectives

- To explains the techniques for creating 3d objects from 2d images positioned at a different interval in space with specialized software.

::::::::::::::::::::::::::::::::::::::::::::::::

## Processing overview

Multiple 2d photographs can be used to generate [point clouds](https://en.wikipedia.org/wiki/Point_Cloud) where each point has now three-dimensional coordinates.

![*Dense Point Cloud example of a small object*](https://data.d4science.org/shub/E_aFlFV0paV3RQaGQwTkJrTGVpd0pVVktJdFpEeXh4b2gySU8yMjJTNGJybFc4Z2JNS2tqWm5raHRXK0U4VHFVRA==)


These points can be further use to create a 3d meshes by mean of [another type of triangulation](https://en.wikipedia.org/wiki/Mesh_generation). 

![*3d Mesh of a small object*](https://data.d4science.org/shub/E_a0hoL2Y2dmZpREorYjNGTkx3QXBGcnZoQUd5NlhIVHQ0eStLZkVMd0hXN2RhckxRMDM5dG9ralpMaFFlSEs4cg==)


The mesh can then be [texture mapped](https://en.wikipedia.org/wiki/Texture_mapping) for the final realistic appearance of the studied subject.

![*3d Mesh of a small object with texture*](https://data.d4science.org/shub/E_ZEp0UkZxbFFvdUVXN29QMmtqWldTdDlBRnBhZUdUcTBPZUhJbG44ZEFLOEsxM2R5dlBZaE1yUG9XVUZzcHZBVg==)

<iframe src="https://gltf-viewer.donmccurdy.com#kiosk=1&model=https://data.d4science.org/shub/E_ZXp0WWx5S3JiVjE2RFc3WkVoMjhJSlUyUmpCWUFEQUdCSVlqamY2aC9zRUVGdWZLYWRVV0Vwem0xMHRiRkYwWQ==" style="width: 100%;" height="400px" frameBorder="0"></iframe>




<!--Underlying technology is more familiar
that we think! We can happily ignore 
the concepts and formulas used 
in the software. 

But it is useful to be aware of what it works.-->

|   |
|---|
### Basic steps in the processing phase

**1. Feature detection** (originally performed manually but now performed automatically by the algorithm of the software)

**2. Feature matching** (originally performed manually but now performed automatically by the algorithm of the software)

**3. Structure reconstruction** (performed automatically by the algorithm of the software)


### Features detection

Features are "interest points" or 
"key points" in an image. 
The goal of this step is to find 
points which are repeatable and distinctive.
Corners and other distinctive patterns 
in the image are obvious features to consider.

:::::::::::challenge 

## Try it yourself?
Open this image in Gimp or others photo editing software and try to recognize 6 or more distinctive features.

[Match-1](https://data.d4science.org/shub/E_VGNNb0R2VVltRmxaOHlhSXZnczIrTkZkL1ZUUXZlTElBLzBWTHUzenREdXZSb1RMcXNwdDBNS1Qwb2d3aWNnWQ==)

What points would you choose?
:::::::
:::::::::::::solution
[Here](https://data.d4science.org/shub/E_WW9zZUluVUxmVzJFRlpDcFV3UE5MeHVJNU96d25LWlJDdDhZZlJSQnpjcWptZVowRS9YcGxHWHZUN0RmLzVlSQ==) you can download an image of the possible solution. You will need to zoom into the image to see the exact feature points.

::::::::::::::::::::

 
### Features matching

Find correspondences of features across 
different views. 
The goal of this step is to 
detect (at least some of) 
correspondence between features in 
two or more images.


:::::::::::challenge 

## Try it yourself?

Open this image in Gimp or others photo editing software and try to recognize 6 or more features already found in the previous image.

[Match-2](https://data.d4science.org/shub/E_NXBISUtZTnhDbHVGNHNxUXh0cEQzSGVldFVPMEtWWisyVU8xVmFCWWliTTNEQWIwNGx2VldUQ0xhWUZOMkk2SA==)


Do the features below correspond with each other?

:::::::
:::::::::::::solution
[Here](https://data.d4science.org/shub/E_WFZSR0Z0Y29CTzNMNmVTdWNxelZqdFc2bkxOV3VuWU1nc0ViMVQ2MVU3RmtVMGZYd1NWclU4b24zWjB6R3VTUA==) you can download an image of the solution. You will need to zoom into the image to see the exact feature points.
::::::::::::::::::::

### Structure reconstruction

<!--Load all extracted features from an 
initial pair of images. Builds a 
projection of the points in 3D space by using the camera position.-->

The software will recognize the features from all the loaded images. Builds a 
projection of the points in 3D space by using the camera position.

The scene is incrementally 
extended by adding new images and 
triangulating new points. A much denser set of features is produced.

The output of this process is 
a "point cloud" or a [collection of points](#definition). 
The 3D model is created by creating a [triangular mesh](#definition). The texture is then mapped to the [surface](#definition).


![*Matching features in multiple images*](https://data.d4science.org/shub/E_bU9MSEZaRGpOaGFJZ2hsL1dCWi85U0NZbUJiVDh5YlBlUmxmTGI3UE9ic1dvOEdkOGFpS3JnYmRrelYrY0JOaQ==)

We can apply a mask to the whole sets of images so that the algorithm does not have to calculate the points that are not interested. In this case when using turntables is recommended to shoot always one image without the object.

![*Mask  used in multiple images*](https://data.d4science.org/shub/E_L3Y4dlE2Rm9ZVU1BcCtSaHFoS1A5UHZpUEpXYVdaK2tRNm9MOGdjT1Y0YXE3bkdvR2FTdU1MSlp1R3ozRVVwYg==)


![*Reconstructed model from matching features in multiple images*](https://data.d4science.org/shub/E_dUFhdW4vd2x0SVRNOWFjOWVjc1pwU2FyaFJXWlA2VnBzWjI1QnN5L3UvalMxYlVVSFhkMmEwb0FqTlkrdHZaMg==)

\
\

:::::::::::::::::::::::::::::::::::::: questions 

- What software is available to create 3d models after image acquisition?
::::::::::::::::::::::::::::::::::::::::::::::::

::::::::::::::::::::::::::::::::::::: objectives

- Explains the techniques for using specific software for processing the images and for creating 3d models.
::::::::::::::::::::::::::::::::::::::::::::::::
<!--
## Processing the images

Once the photographs have been acquired, 
the next step is to transfer the images 
to a PC (refer to [setup for specifications][setup]).


Whatever processing you apply to images 
(e.g. brightness, colour adjustments) 
should be applied to the whole dataset. 
Otherwise you will have alignment problems. 

Resizing, rotating and transforming the 
geometry of the images should not be applied.
-->


## Preparing the images

Once the photographs have been acquired, 
the next step is to transfer the images 
to a PC (refer to [setup][setup] for specifications).

Please use the software we downloaded in the setup section [Raw Therapee](https://www.3dflow.net). With this free software you can convert the images from the raw file format to various other formats. The raw file format of different cameras is probably already the best file format you can use during the processing of the images in the photogrammetry software, because it retains the exif data, the most accurate color range and the best resolution. However not all the photogrammetry software are able to read the different type of raw formats of the different camera types. In this case you will need to use software such as Raw therapy (which you can  also use to re-organize your files) to convert them in a more readable format. Usually the uncompressed Tiff or Tif file format is a good choice, because many photogrammetry software can read it and it will retain good quality information, including all the ones of the camera at the moment of shooting.

Whatever file format you use it must be readable form the software you are about to use for reconstructing the models and you must be sure you choose a format that retains the Exif information within the file.
if you are not sure if your file has the information needed there are lots of online tools that can provide such information. One example is [ExifInfo.org](https://exifinfo.org/).
(Raw Therapee can also provide this information on the info panel however you should always check at least one of the image after exporting them from Raw therapy.

> For this lessons the images that you have downloaded from the [examples data sets](setup) are already converted for you and, although it is better  for you to get use to software for batch converting images, at this stage you will not need to take further actions after downloading them*




 
<!--
### Deleting the background 

If you wish to delete the 
background in the images, open one of the 
images that contain a large area of the 
background cloth in the image processing
software. 

Use the colour picker (circled in red) to select 
an area of the cloth. 

Click with the right mouse button on the 
background to select that area.

Go to "save as" and save it in the 
main project folder (NOT the images folder).

## Photogrammetry Software Workflow

The following instructions are specific
to 3DF Zephir.

Go to the workflow menu and choose *New Project*
 
### Import Images 

Browse to the folder that contains your 
images and click *Select Folder*.

Click *Create camera from each file*. 
The number of cameras that appear 
in the menu will depend on how
many images you have taken. 


Save the project in the project root folder. 

### Import Masks

Select the tools menu, 
go to import and select *Import Masks*.
 
 
Select *From background* in the Method drop down box.

Select *Replacement* in the *Operation* drop down box. 

In the *Filename template* select type in the 
name that you saved earlier in paint. 

Under tolerance, select a number from 30 to 50. 
The idea is that for each point in the image, 
it will compare it to the colour in our background 
mask image at the same point. If it is the same, 
it will not be included in the 3D model. 

The tolerance number is the amount that you 
expect the colour to vary to the background image. 
The larger the number, the larger it will allow 
a deviation from the colour to be masked out. 

Click *OK*. Then select the folder that contains the image.
 

Browse to the appropriate folder and 
click *Select Folder*.

Wait for the masks to be processed. 
The time to complete will depend on the number 
of images that you have taken.
 
 
Double click each image and look for any areas that 
have been masked out that shouldn’t have been. 
In this example, it has masked out the back of 
the model. 
Use either the rectangle or the loop tool 
(circled in red) to select the area.

 
Right click and choose *Subtract Selection*. 
This will remove the selection from the mask. 
Repeat this for all images.

Save the project so far.


### Align Photos

The next step is to align the photos. For
this, go to *Workflow* in the menu
and select *Align Photos*.
 
Select the options you want, 
but make sure the *Constrain features by mask* 
is selected. Then click *OK*.
 
 
Save the project so far. 

### Build dense cloud

Go to *Workflow* in the menu
and select *Build Dense Cloud*.


Choose the options you want and click *OK*.

Save the progress so far. 

### Build mesh

Go to *Workflow* in the menu
and select *Build Mesh*.

Choose your preferred options. 
Make sure *Source data* has *Dense cloud*
selected. 
Then click OK.
 

This process will produce a 3D model. 
The 3D model can be exported, 
or edited within the scene. 

### Build Texture

The final, optional step is to re-project 
the texture onto the 3D surface. 
This makes the photographic quality much better.


Go to *Workflow* in the menu
and select *Build Texture*.


Choose the following options. 
They are suitable for most models. 
Then click *OK*.
 
Now you will have a 3D model with the texture. -->


## Photogrammetry Software Workflow



### Organizing the workspace

Using a suitable name which reflects your project, transfer all
images into a folder.

Good practice includes:

```
		ResourceIDifExistent_NameofObject_DateProcessedinFormatYY.MM.DD
```

Within this folder, create another 
one named images. 
Copy the images from the camera into the images folder.

The following instructions are specific
to [3DF Zephir](https://www.3dflow.net/3df-zephyr-photogrammetry-software/).

Go to the workflow menu and choose **New Project**, you will be presented with a the *"New project wizard window"*.

Choose the first box *Sparse* in order to go trough the all process manually. Click **Next>** you will be presented with the  *"Photos selection page"* .
 
### Importing Images 

- Browse to the folder that contains your 
images and click **Select Folder** or Select a your **Single Images**. Click **Next>**
- you will be presented with the  *"Camera calibration page*". If you have a separate Exif file for calibrating the camera you can add it here, and you can also manually calibrate you camera in the *"Modify Calibration page"* otherwise go on and click **Next>**


![*Original photo*](https://data.d4science.org/shub/E_WHBodVMwdUZ5TXU3Z3h3MDhrQjBIMmFybkNGRXN6aEtqMUNuUWY2QlFKVmF5V1hxbzRFaDd6Q0ZIT0NsZ1BaNQ==)



### Importing Masks (optional)

In the *"Photos selection page"* there is an option to import the mask, if selected a new option will be presented and a new tool called **Masquerade** will be available before importing the images.
Within this tool (which is also available from the main interface), it will be possible to generate a Mask to apply to all the images.
The tools is quite simple to use so that if you want to try to apply a mask you can use a sample image provided in the sets of the downloaded dataset as a first file.

![*Original photo of the mask*](https://data.d4science.org/shub/E_RFF0RkVickVCSmJCSHFtZm10MEYwNXFCN2xtY2JSVmY5MmxHVlo4a01WRUppcW9GK1Z0UzlXeHJXK1hJR3pwQQ==)


### Aligning Photos

The next step is to align the photos. For
this:

- you will be presented with the *"Camera orientation page"*. Keep the general setting and click **Next>**
- you will be presented with the  *"Start reconstruction"* page. Click **Run**
- you will be presented with the *"Reconstruction Successful page"*. Click **Finish**
- Save the project in the same [folder](# Create a folder) created before.

*"Once the camera orientation phase has been completed, the sparse point cloud will appear in the workspace as well as the oriented cameras identified by blue pyramids."* Now you can familiarize with the navigation of the 3d space and the interface. For example go to **Scene-> Bounding Box-> Edit Bounding box** and limit the created sparse cloud within the the bounding box.This will speed-up the process when creating the final mesh.

![*Sparse Point Cloud*](https://data.d4science.org/shub/E_dEtEY3RCZFYyVjMxMjNrOEcvYUxyUENieHpZZUdWc0g1TEVVVUtUTFhWTTRXZ0JON21tRkVKN2ZCREVjcFRxcA==)


### Build dense cloud (optional)
The next step is to create a Dense PointCloud. For
this:

- Go to *Workflow* in the menu and select *Advanced-> Dense Point Cloud Generation*.
- you will be presented with the  *"Dense Point Cloud Generation wizard"*. **Select All Cameras** and click **Next>**
- you will be presented with the  *"Dense Point Cloud Creation"* page. Leave the general settings and click **Next>**
- you will be presented with the  *"Start Densification"* page. Click **Run**
- when finished you will be presented with the *"Dense Point Cloud generation successful"* page, click **Finish**
- Save the project in the same [folder](# Create a folder) created before.

![*Dense Point Cloud*](https://data.d4science.org/shub/E_enEwZ01YYXFVdVlrL3NGbXQrWkoxM2VOYk1sQkY3VVI1L014RUV5UjJsMGwvYWZvcU85endtdXpzU3A1OUpleA==)


### Cleaning the dense cloud (optional)
Before trying to create the final mesh we should delete all the unwanted points that where generated within the bounding box. We could do that by using the same bounding box to restrict even more the area where the algorithm is going to be applied for the triangulation. However in order to be accustom with the software interface, we will delete all the unecessary points manually.

- Go to the *Editing panel* on your right and choose **By Hand**. Choose **Poly** and  **Remove**.
- Start selecting the points that you do not need and once selected deleted them  with the del key.
- Once happy save the project in the same [folder](# Create a folder) created before.



### Building the mesh
The next step is to create a Dense PointCloud. For
this:

- Go to *Workflow* in the menu and select *Advanced-> Mesh Extraction*
- you will be presented with the  *"Mesh Generation wizard"*. **Drop Down** the name of your dense point cloud, **Select All Cameras** and click **Next>**
- you will be presented with the *"Surface Reconstruction"* page. Leave the general settings and click **Next>**
- you will be presented with the  *"Start Mesh Creation"* page. Click **Run**
- when finished you will be presented with the *"Mesh Creation successful"* page, click **Finish**. This process will produce a 3D model.
- Once happy save the project in the same [folder](# Create a folder) created before.

![*High Resolution Mesh*](https://data.d4science.org/shub/E_eEIvTkZMYWdoM3pySDdjZUtjU0J1NGFya29vMDVyQ1ErQ0k5eHh1TVlXZkcxaVBUL21ydGlEa1NPeXU0UUhzZQ==)

### Building the Texture

The final step is to re-project the texture onto the 3D surface.For this:

- Go to *Workflow* in the menu and select *Textured Mesh Generation*
- you will be presented with the  *"Textured Mesh Generation wizard"*. **Drop Down** the name of your mesh, **Select All Cameras** and click **Next>**
- you will be presented with the *"Texturing"* page. Leave the general settings and click **Next>**
- you will be presented with the  *"Textured Mesh Generation wizard"* page. Click **Run**
- when finished you will be presented with the *"Textured Mesh Generation wizard result"* page, click **Finish**. Now you will have a 3D model with the texture.
- Save the project in the same [folder](# Create a folder) created before.


![*High Resolution Mesh Texture*](https://data.d4science.org/shub/E_R21uT3hnMGRjeGZ0WHVZUkgwTW9FLzFTYzJIaWYvVGY4RWltQ1ZkZmRiZTlndExkMEtPdGRsQll3N0UzZnBVbw==)


### Exporting the mesh with textures for High-Res visualization
At this point we need to export an high resolution mesh for different purposes. For this:

- Go to *Export* in the menu and select *Export Textured Mesh*. **Drop Down** the name of your mesh, **Drop Down** your preferred format and click **Export**
- create another folder called *"Exports"* within the same folder of the images and save the model in this folder.


![https://data.d4science.org/shub/E_Zk92OG5TUEN5ZGx1ais1WS80UWdVVEZORGRDKzl1YjNLR2syMWZYY3JFcTBBVGhQTSs5MjdFZTI4NVR3U2p1ZQ==](https://data.d4science.org/shub/E_Zk92OG5TUEN5ZGx1ais1WS80UWdVVEZORGRDKzl1YjNLR2syMWZYY3JFcTBBVGhQTSs5MjdFZTI4NVR3U2p1ZQ==)*High Resolution Mesh Textured*

### Exporting the mesh with textures for online publishing

At this point we need to export the model at a lower resolution mesh for online publishing. For this:

- Select your textured mesh in the right window *"Textured Meshes"* **Right Click** on it and select **Clone**. A copy of your mesh will be created.
- Go to *Tools* in the menu and select *Mesh Filters-> Decimatiom*. You will be presented with the *"Mesh decimation"* small window. **Drop Down** the name of your second mesh, select *preserve boundaries* and *Apply Filter*

At this point we need to regenerate the texture for the lower resolution mesh. To do so we need to repeat the process above:

- Go to *Workflow* in the menu and select *Textured Mesh Generation*
- you will be presented with the  *"Textured Mesh Generation wizard"*. **Drop Down** the name of your new mesh, **Select All Cameras** and click **Next>**
- you will be presented with the *"Texturing"* page. Leave the general settings and click **Next>**
- you will be presented with the  *"Textured Mesh Generation wizard"* page. Click **Run**
- when finished you will be presented with the *"Textured Mesh Generation wizard result"* page, click **Finish**. Now you will have the new low resolution 3D model with the texture.
- Save the project in the same [folder](# Create a folder) created before.
- Go to *Export* in the menu and select *Export Textured Mesh*. **Drop Down** the name of your second mesh, **Drop Down** the format ***.glb*** or ***.gltf*** and click **Export**
- create another folder called *"Exports"* within the same folder of the images and save the model in this folder.

<iframe src="https://gltf-viewer.donmccurdy.com#kiosk=1&model=https://data.d4science.org/shub/E_azJzMVp6MENORnRUd0FEdElCa3g5WVBIdEQ5cldBUlJwOHkyYjRITHpTYmVUcFdIUDc1VzRhWTFGdWc5SytNVA==" style="width: 100%;" height="400px" bgColor="#dbdbdb" frameBorder="0"></iframe>

\
\


### Adding real world scale

\
\

### Final Remarks

During the whole process you will encounter more options and setting then the ones described above. You can find a more complete and technical advice in [this document](https://www.3dflow.net/zephyr-doc/en/Extractingadensepointcloud.html).

Or if you prefer it as a PDF file you can find it [here](https://3df-eu.fra1.digitaloceanspaces.com/zephyr-doc/3DF%20Zephyr%20Manual%207.500%20English.pdf)

As well as a series of video tutorials on [this page](https://www.3dflow.net/technology/documents/3df-zephyr-tutorials/).

You can also find advice and specific topic help in the official [forum](https://www.3dflow.net/forums/) of the software.

Finally you can also join the [Discord Channel](https://discord.com/invite/3HMUKff) if you prefer.

<!--
---
title: 'Links'
teaching: 10
exercises: 2
---
-->

## Additional Links

\

### Theory

Image theory

- [Cambridge in Color](https://www.cambridgeincolour.com/)
-	[Images and content from James Hays, Computer Vision module @ Brown University](http://cs.brown.edu/courses/cs143/)
<!--
-	[Comparison of methods](http://www.stporter.com/wp- content/uploads/2016/04/A_Comparison_of_Methods_for_Creating_3D.pdf)
-	[Tips and Tricks](http://www.agisoft.com/support/tips-tricks/)-->

Photogrammetry

- [CHI](https://culturalheritageimaging.org/Technologies/Photogrammetry/)
- [ADS](https://archaeologydataservice.ac.uk/help-guidance/guides-to-good-practice/data-collection-and-fieldwork/close-range-photogrammetry/data-collection-and-documentation/typical-steps-for-a-crp-project/)
-	[Tips for setup](https://blog.sketchfab.com/how-to-set-up-a-successful-photogrammetry-project/)
-	[Tips for taking photos](http://www.tested.com/art/makers/460142-art-photogrammetry-how-take-your- photos/)


### Principles and Guidelines
- [Basic principles and tips for 3D digitisation of cultural heritage -European Commission](https://digital-strategy.ec.europa.eu/en/library/basic-principles-and-tips-3d-digitisation-cultural-heritage)
- [Share 3D Carare](https://carare.gitbook.io/share-3d-guidelines/)

### Other tutorials

- [Historic England's Photogrammetry tutorial](https://historicengland.org.uk/images-books/publications/photogrammetric-applications-for-cultural-heritage/heag066-photogrammetric-applications-cultural-heritage/)
- [Alicevision](https://github.com/alicevision/meshroom/wiki/Tutorials)

### Tools and software

- [Exif checker](https://exifinfo.org/)
- [ADS 3D viewer](https://archaeologydataservice.ac.uk/about/projects/ads-3d-viewer/)
- [Meshroom](https://alicevision.org/#meshroom)
- [Metashape](https://www.agisoft.com/)
 
<!--

### Additional Links

-	[Images and content from James Hays, Computer Vision module @ Brown University](http://cs.brown.edu/courses/cs143/)
-	[Comparison of methods](http://www.stporter.com/wp- content/uploads/2016/04/A_Comparison_of_Methods_for_Creating_3D.pdf)
-	[Tips and Tricks](http://www.agisoft.com/support/tips-tricks/)
-	[Tips for setup](https://blog.sketchfab.com/how-to-set-up-a-successful-photogrammetry-project/)
-	[Tips for taking photos](http://www.tested.com/art/makers/460142-art-photogrammetry-how-take-your- photos/)

-->


<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>Photogrammetry Digitisation: All in One View</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" type="text/css" href="assets/styles.css">
<script src="assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="site.webmanifest">
<link rel="mask-icon" href="safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
</head>
<body>
    <header id="top" class="navbar navbar-expand-md navbar-light bg-white top-nav incubator"><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-6">
      <div class="large-logo">
        <img alt="Carpentries Incubator" src="assets/images/incubator-logo.svg"><abbr class="badge badge-light" title="This lesson is in the beta phase, which means that it is ready for teaching by instructors outside of the original author team." style="background-color: #001483; border-radius: 5px">
          <a href="https://cdh.carpentries.org/the-lesson-life-cycle.html#polishing-beta-stage" class="external-link alert-link" style="color: #FFF7F1">
            <i aria-hidden="true" class="icon" data-feather="alert-circle" style="border-radius: 5px"></i>
            Beta
          </a>
          <span class="visually-hidden">This lesson is in the beta phase, which means that it is ready for teaching by instructors outside of the original author team.</span>
        </abbr>
        
      </div>
    </div>
    <div class="selector-container">
      
      
      <div class="dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Learner View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1">
<li><button class="dropdown-item" type="button" onclick="window.location.href='instructor/aio.html';">Instructor View</button></li>
        </ul>
</div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl navbar-light bg-white bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="assets/images/incubator-logo-sm.svg">
</div>
    <div class="lesson-title-md">
      Photogrammetry Digitisation
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="search button" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item">
          <span class="lesson-title">
            Photogrammetry Digitisation
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="reference.html#glossary">Glossary</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="profiles.html">Learner Profiles</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
<li><a class="dropdown-item" href="reference.html">Reference</a></li>
          </ul>
</li>
      </ul>
</div>
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
<input class="form-control me-2 searchbox" type="search" placeholder="Search" aria-label="Search"><button class="btn btn-outline-success tablet-search-button" type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="search button"></i>
        </button>
      </fieldset>
</form>
  </div>
<!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Photogrammetry Digitisation
</div>

<aside class="col-md-12 lesson-progress"><div style="width: %" class="percentage">
    %
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: %" aria-valuenow="" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Learner View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="instructor/aio.html">Instructor View</a>
                      </div>
                    </div>
                  </div>
<!--/div.accordion-item-->
                </div>
<!--/div.accordion-flush-->
              </div>
<!--div.sidenav-view-selector -->
            </div>
<!--/div.col -->
      
            <hr>
</div>
<!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Setup</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="CHDigitization.html">1. Introduction to Methods</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="introduction.html">2. Photogrammetry</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="360Panorama.html">3. 360 Panorama</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width">
<div class="accordion accordion-flush resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul>
<li>
                        <a href="key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="reference.html#glossary">Glossary</a>
                      </li>
                      <li>
                        <a href="profiles.html">Learner Profiles</a>
                      </li>
                      <li><a href="reference.html">Reference</a></li>
                    </ul>
</div>
                </div>
              </div>
            </div>
            <hr class="half-width resources">
<a href="aio.html">See all in one page</a>
            

            <hr class="d-none d-sm-block d-md-none">
<div class="d-grid gap-1">
            
            </div>
          </div>
<!-- /div.accordion -->
        </div>
<!-- /div.sidebar-inner -->
      </nav>
</div>
<!-- /div.sidebar -->
  </div>
<!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-extra.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <main id="main-content" class="main-content"><div class="container lesson-content">
        
        
<section id="aio-CHDigitization"><p>Content from <a href="CHDigitization.html">Introduction to Methods</a></p>
<hr>
<p>Last updated on 2024-01-16 |
        
        <a href="https://github.com/culturedigitalskills/2023-digitisation-photogrammetry/edit/main/episodes/CHDigitization.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li><p>What is Cultural Heritage digitization?</p></li>
<li><p>What are the different techniques for digitizing cultural
objects?</p></li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li><p>Explains what is Cultural Heritage digitization</p></li>
<li><p>Shows various techniques that can be applied to capture real
world objects and environments.</p></li>
</ul>
<!-- - Advantages and disadvantages for the use of this techniques.-->
</div>
</div>
</div>
</div>
</div>
<blockquote>
<p>Many methods exist that aim to capture 3d data in Cultural Heritage.
All have their own advantages and disadvantages compared to
photogrammetry. Although it would be quite interesting to explore more
in depth the differences between these methods and when appropriately
choose them depending on the working scenario, this is out of the scope
of this workshop. Nevertheless here you can find a list of some of other
methods that can be used to produce 3 dimensional data from real world
objects and environments.</p>
</blockquote>
<p><br><br></p>
<div class="section level3">
<h3 id="introduction">
<strong>Introduction</strong><a class="anchor" aria-label="anchor" href="#introduction"></a>
</h3>
<p>Cultural Heritage Digitisation is the art and science of…</p>
<p><br><br></p>
</div>
<div class="section level3">
<h3 id="laser-based-scanners-long-medium-short-range">
<strong>Laser based scanners (long, medium, short
range)</strong><a class="anchor" aria-label="anchor" href="#laser-based-scanners-long-medium-short-range"></a>
</h3>
<p>There are various methods used in laser scanning (Time of flight,
Phase shift, Triangulation). All of them are applied in different
situations depending on the distance of an object to record and also
depending on the condition of the environment. The most common method
when recording long and medium range is called LIDAR (Light Detection
and Ranging) which is applied usually when recording large landscapes or
buildings. In this case the sensor on the laser calculates the time that
it takes for the light of the laser to return to the sensor thus
providing 3d coordinates of each recorded point. When recording
individual objects instead, short range scanners are more used applying
triangulation techniques which return to the sensor in a specific
location.</p>
<table class="table">
<colgroup>
<col width="50%">
<col width="50%">
</colgroup>
<tbody>
<tr class="odd">
<td><img src="https://upload.wikimedia.org/wikipedia/commons/4/4b/Cyark_Tudor_Place_3.jpg" alt="Cyark Tudor Place 3" class="figure"></td>
<td><img src="https://upload.wikimedia.org/wikipedia/commons/8/85/FARO_Laserscanner_LS.JPG" style="width:60.0%" alt="FARO Laserscanner LS" class="figure"></td>
</tr>
<tr class="even">
<td>3D Laser Scan data cutaway of the interior of Tudor Place. The
Parlour, Saloon and the Drawing Room can be seen in this scan data,
Public domain, CyArk, under <a href="https://creativecommons.org/licenses/by-sa/3.0" class="external-link">CC BY-SA 3.0</a>,
via <a href="https://commons.wikimedia.org/wiki/File:Cyark_Tudor_Place_3.jpg" class="external-link">Wikimedia
Commons</a>
</td>
<td>FARO Laser Scanner LS, FARO Technologies, under <a href="https://creativecommons.org/licenses/by-sa/3.0" class="external-link">CC BY-SA 3.0</a>,
via <a href="https://commons.wikimedia.org/wiki/File:FARO_Laserscanner_LS.JPG" class="external-link">Wikimedia
Commons</a>
</td>
</tr>
</tbody>
</table>
<table class="table">
<colgroup>
<col width="50%">
<col width="50%">
</colgroup>
<tbody>
<tr class="odd">
<td><img src="https://upload.wikimedia.org/wikipedia/commons/f/ff/Mtm-05277e_3d.png" alt="Elevation model of Tithonium Chasma" class="figure"></td>
<td><img src="https://upload.wikimedia.org/wikipedia/commons/2/2c/Yellowscan_LIDAR_on_OnyxStar_FOX-C8_HD.jpg" alt="Yellowscan LIDAR on OnyxStar FOX-C8 HD" class="figure"></td>
</tr>
<tr class="even">
<td>3D view of elevation model of Tithonium Chasma, Public domain, USGS,
under <a href="https://creativecommons.org/licenses/by-sa/3.0" class="external-link">CC BY-SA
3.0</a>, via <a href="https://commons.wikimedia.org/wiki/File:Mtm-05277e_3d.png" class="external-link">Wikimedia
Commons</a>
</td>
<td>LIDAR survey being performed with a Yellowscan LIDAR on the OnyxStar
FOX-C8 HD from AltiGator in November 2015, Belgium, Public Domain, USGS,
under <a href="https://creativecommons.org/licenses/by-sa/4.0" class="external-link">CC BY-SA
4.0</a>, via <a href="https://commons.wikimedia.org/wiki/File:Yellowscan_LIDAR_on_OnyxStar_FOX-C8_HD.jpg" class="external-link">Wikimedia
Commons</a>
</td>
</tr>
</tbody>
</table>
<table class="table">
<colgroup><col width="100%"></colgroup>
<tbody>
<tr class="odd">
<td><img src="https://upload.wikimedia.org/wikipedia/commons/2/26/VIUscan_handheld_3D_scanner_in_use.jpg" alt="VIUscan handheld 3D scanner in use" class="figure"></td>
</tr>
<tr class="even">
<td>Making a 3D-model of an Viking belt buckle using a hand held VIUscan
3D laser scanner. As the device uses a laser scanner to create a 3D
model it also uses a camera to accurately texture map the object,
Creative Tools from Halmdstad, Sweden, under <a href="https://creativecommons.org/licenses/by-sa/2.0" class="external-link">CC BY-SA 2.0</a>,
via <a href="https://commons.wikimedia.org/wiki/File:VIUscan_handheld_3D_scanner_in_use.jpg" class="external-link">Wikimedia
Commons</a>
</td>
</tr>
</tbody>
</table>
<blockquote>
<p>In-depth theory on laser scanning is out of the scope of this
workshop and a more comprensive picture of what is laser scanning can be
found at the <a href="https://archaeologydataservice.ac.uk/help-guidance/guides-to-good-practice/data-collection-and-fieldwork/laser-scanning-for-archaeology/introduction/what-is-laser-scanning/" class="external-link"><strong>ADS
website</strong></a> along its usage for cultural heritage and
archaeological environments.</p>
</blockquote>
<p><br><br></p>
</div>
<div class="section level3">
<h3 id="structured-light-scanners">
<strong>Structured light scanners</strong><a class="anchor" aria-label="anchor" href="#structured-light-scanners"></a>
</h3>
<p>This method uses a more affordable way of recording 3d object by
projecting a pattern onto the object. The canera(S) will record the
distorted pattern on a non-flat surface thus determing the 3d shape of
the oject.</p>
<table class="table">
<colgroup>
<col width="50%">
<col width="50%">
</colgroup>
<tbody>
<tr class="odd">
<td><img src="https://upload.wikimedia.org/wikipedia/commons/0/09/Input_image_for_3D_scanner.png" style="width:75.0%" alt="Input image for 3D scanner" class="figure"></td>
<td><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/aa/3-proj2cam.svg/1920px-3-proj2cam.svg.png?20080805004252" alt="3-proj2cam LS" class="figure"></td>
</tr>
<tr class="even">
<td>A structured-light 3D scanner is a 3D scanning device for measuring
the three-dimensional shape of an object using projected light patterns
and a camera system, Public domain, Hesameh, under <a href="https://creativecommons.org/licenses/by-sa/4.0" class="external-link">CC BY 4.0</a>, via
<a href="https://commons.wikimedia.org/wiki/File:Input_image_for_3D_scanner.png" class="external-link">Wikimedia
Commons</a>
</td>
<td>3-proj2cam, under <a href="https://creativecommons.org/licenses/by-sa/3.0" class="external-link">3-proj2cam, CC
BY-SA 3.0</a>, via <a href="https://commons.wikimedia.org/wiki/File:3-proj2cam.svg" class="external-link">Wikimedia
Commons</a>
</td>
</tr>
</tbody>
</table>
<p><br><br></p>
</div>
<div class="section level3">
<h3 id="arm-based-scanners">
<strong>Arm-based scanners</strong><a class="anchor" aria-label="anchor" href="#arm-based-scanners"></a>
</h3>
<p>This types of scanner are manually operated by tracing the object for
digitization. They are quite time consuming and deployed when a very
precise digitization is required, such as small fragments of pottery and
other small objects.</p>
<table class="table">
<colgroup><col width="100%"></colgroup>
<tbody>
<tr class="odd">
<td><img src="https://upload.wikimedia.org/wikipedia/commons/3/39/Kreonace725.jpg" style="width:50.0%" alt="Kreonace725" class="figure"></td>
</tr>
<tr class="even">
<td>7-axis measurement arm with a laser scanner attached, Public domain,
TestoCH, under <a href="https://creativecommons.org/licenses/by-sa/4.0" class="external-link">CC BY 4.0</a>, via
<a href="https://commons.wikimedia.org/wiki/File:Kreonace725.jpg" class="external-link">Wikimedia
Commons</a>
</td>
</tr>
</tbody>
</table>
<p><br><br></p>
</div>
<div class="section level3">
<h3 id="mri-magnetic-resonance-imaging">
<strong>MRI (Magnetic Resonance Imaging)</strong><a class="anchor" aria-label="anchor" href="#mri-magnetic-resonance-imaging"></a>
</h3>
<p>This type of scanning records the exterior and the interior of an
object and is used mostly in medicine to record or full body or body
parts, however it can also be used to record pottery if volumetric data
are required.</p>
<table class="table">
<colgroup>
<col width="50%">
<col width="50%">
</colgroup>
<tbody>
<tr class="odd">
<td><img src="https://upload.wikimedia.org/wikipedia/commons/9/9a/US_Navy_110427-N-2531L-135_Tori_Randall%2C_Ph.D._prepares_a_550-year_old_Peruvian_child_mummy_for_a_CT_scan.jpg" alt="US Navy 110427-N-2531L-135 Tori Randall, Ph.D. prepares a 550-year old Peruvian child mummy for a CT scan" class="figure"></td>
<td><img src="https://upload.wikimedia.org/wikipedia/commons/7/78/US_Navy_110427-N-YY999-001_A_CT_scan_of_a_Peruvian_mummy_taken_at_Naval_Medical_Center_San_Diego_provides_details_of_the_muscular_and_skeletal_stru.jpg" alt="US Navy 110427-N-YY999-001 A CT scan of a Peruvian mummy taken at Naval Medical Center San Diego provides details of the muscular and skeletal stru" class="figure"></td>
</tr>
<tr class="even">
<td>SAN DIEGO (April 27, 2011) Tori Randall, Ph.D., curator for the
Department of Physical Anthropology at the San Diego Museum of Man,
prepares a 550-year old Peruvian child mummy for a CT scan at Naval
Medical Center San Diego. The medical center is the only medical
facility in San Diego County with a Flash Dual Source 128 CT scanner
that is Dual Energy capable. This unique capability uses two different
energy sources to differentiate characteristics in tissue and bone
beyond conventional CT imaging. (U.S. Navy photo by Mass Communication
Specialist 3rd Class Samantha A. Lewis/Released), Public domain, Mass
Communication Specialist 3rd Class Samantha A. Lewis under <a href="https://creativecommons.org/licenses/by-sa/4.0" class="external-link">CC BY 4.0</a>, via
<a href="https://commons.wikimedia.org/wiki/File:US_Navy_110427-N-2531L-135_Tori_Randall,_Ph.D._prepares_a_550-year_old_Peruvian_child_mummy_for_a_CT_scan.jpg" class="external-link">Wikimedia
Commons</a>
</td>
<td>SAN DIEGO (April 27, 2011) A CT scan of a Peruvian mummy taken at
Naval Medical Center San Diego provides details of the muscular and
skeletal structure. The mummy is part of a planned exhibit at the San
Diego Museum of Man. The medical center is the only medical facility in
San Diego County with a Flash Dual Source 128 CT scanner that is Dual
Energy capable. This unique capability uses two different energy sources
to differentiate characteristics in tissue and bone beyond conventional
CT imaging. (U.S. Navy photo/Released), Public Domain, U.S. Navy photo,
under <a href="https://creativecommons.org/licenses/by-sa/3.0" class="external-link">CC BY
3.0</a>, via <a href="https://commons.wikimedia.org/wiki/File:US_Navy_110427-N-YY999-001_A_CT_scan_of_a_Peruvian_mummy_taken_at_Naval_Medical_Center_San_Diego_provides_details_of_the_muscular_and_skeletal_stru.jpg" class="external-link">Wikimedia
Commons</a>
</td>
</tr>
</tbody>
</table>
<p><br><br></p>
</div>
<div class="section level3">
<h3 id="dct-3d-computed-tomography">
<strong>(3d)CT (3d Computed Tomography)</strong><a class="anchor" aria-label="anchor" href="#dct-3d-computed-tomography"></a>
</h3>
<p>This method uses Xray in order to cupture volumetric data trough
slides the software then reconstruct the 3d model by assembling all the
slides together.</p>
<table class="table">
<colgroup>
<col width="50%">
<col width="50%">
</colgroup>
<tbody>
<tr class="odd">
<td><img src="https://upload.wikimedia.org/wikipedia/commons/6/68/3d_CT_scan_animation.gif" alt="3d CT scan animation" class="figure"></td>
<td><img src="https://upload.wikimedia.org/wikipedia/commons/8/86/3D_Computed_Tomography_PL.png" style="width:80.0%" alt="3D Computed Tomography PL" class="figure"></td>
</tr>
<tr class="even">
<td>3d CT scan animation, Public domain, under <a href="https://creativecommons.org/licenses/by-sa/4.0" class="external-link">CC BY 4.0</a>, via
<a href="https://commons.wikimedia.org/wiki/File:3d_CT_scan_animation.gif" class="external-link">Wikimedia
Commons</a>
</td>
<td>Schematic representation of the operating principle of 3D computed
tomography, Public domain, Astatine211 translation of original work by
Torsten Brandmüller, under <a href="https://creativecommons.org/licenses/by-sa/3.0" class="external-link">CC BY-SA 3.0</a>,
via <a href="https://commons.wikimedia.org/wiki/File:3D_Computed_Tomography_PL.png" class="external-link">Wikimedia
Commons</a>
</td>
</tr>
</tbody>
</table>
<p><br><br><!--
### **(3d)Ultrasound** ###
This method is used in medicine to record vessels or human fetus. When recorded over time it is referred as 4d ultrasound with three spatial dimension and one time dimension.

|   |   |
|---|---|
| ![3dultrasound 20 weeks](https://upload.wikimedia.org/wikipedia/commons/b/bf/3dultrasound_20_weeks.jpg) | ![CRL Crown rump length 12 weeks ecografia Dr. Wolfgang Moroder](https://upload.wikimedia.org/wikipedia/commons/c/c7/CRL_Crown_rump_length_12_weeks_ecografia_Dr._Wolfgang_Moroder.jpg){width="80%"} | 
| A 3D ultrasound taken of a fetus at 20 weeks, Public domain, Staecker, under [CC BY 3.0](https://creativecommons.org/licenses/by-sa/3.0), via [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:3dultrasound_20_weeks.jpg) |Ultrasound image of the foetus a 30 weeks of pregnancy in a sagittal scan. Measurements of fetal Crown Rump Lenght (CRL), Public domain, Wolfgang Moroder, under [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0), via [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:CRL_Crown_rump_length_12_weeks_ecografia_Dr._Wolfgang_Moroder.jpg) |--></p>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</div></section><section id="aio-introduction"><p>Content from <a href="introduction.html">Photogrammetry</a></p>
<hr>
<p>Last updated on 2024-01-16 |
        
        <a href="https://github.com/culturedigitalskills/2023-digitisation-photogrammetry/edit/main/episodes/introduction.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li><p>What is photogrammetry?</p></li>
<li><p>Where and when can we use photogrammetry?</p></li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li><p>Explains what is photogrammetry</p></li>
<li><p>Shows various scenarios where this technique can be applied to
capture real world objects and environments.</p></li>
</ul>
<!-- - Advantages and disadvantages for the use of this techniques.-->
</div>
</div>
</div>
</div>
</div>
<section id="definition-what"><h2 class="section-heading">Definition (What)<a class="anchor" aria-label="anchor" href="#definition-what"></a>
</h2>
<hr class="half-width">
<p>There are different therms to be aware when referring to
photogrammetry. This therm was coined the first time by Prussian
architect <strong>Albrecht Meydenbauer</strong> and it refers to the act
of measuring the images to understand the 3d position of the represented
object or environment. You can read more about the history <a href="http://www.theulegium.de/fileadmin/user_upload/Texte/Meydenb.pdf" class="external-link"><strong>in
this article</strong></a> by Jörg Albertz. You can also find more in
depth about the use of photogrammetry at the <a href="https://www.isprs.org/" class="external-link"><strong>International Society of
Photogrammetry and Remote Sensing</strong></a></p>
<table class="table">
<colgroup>
<col width="33%">
<col width="33%">
<col width="33%">
</colgroup>
<tbody>
<tr class="odd">
<td><img src="https://upload.wikimedia.org/wikipedia/commons/f/f3/Albrecht_Meydenbauer_%281834-1921%29.jpg" style="width:78.0%" alt="Albrecht Meydenbauer" class="figure"></td>
<td><img src="https://upload.wikimedia.org/wikipedia/commons/b/b3/Nakres_fotogrammetricke_kamery.jpg" style="width:83.0%" alt="Meydenbauer’s" class="figure"></td>
<td><img src="https://upload.wikimedia.org/wikipedia/commons/7/7d/Gelnhausen_Johanniterhof_81-015.jpg" alt="Holztor und der Johanniterhof" class="figure"></td>
</tr>
<tr class="even">
<td>Albrecht Meydenbauer (1834-1921) inconnu, photopile avant 1921,
Public domain, under <a href="https://creativecommons.org/licenses/by-sa/4.0" class="external-link">CC BY-SA 4.0</a>,
via <a href="https://commons.wikimedia.org/wiki/File:Albrecht_Meydenbauer_(1834-1921).jpg" class="external-link">Wikimedia
Commons</a>
</td>
<td>Meydenbauer’s camera developed in 1872, um 1900, Albrecht
Meydenbauer, Public domain, under <a href="https://creativecommons.org/licenses/by-sa/4.0" class="external-link">CC BY-SA 4.0</a>,
via <a href="https://commons.wikimedia.org/wiki/File:Nakres_fotogrammetricke_kamery.jpg" class="external-link">Wikimedia
Commons</a>
</td>
<td>Holztor und der Johanniterhof in Gelnhausen, um 1900, Albrecht
Meydenbauer, Public domain, under <a href="https://creativecommons.org/licenses/by-sa/4.0" class="external-link">CC BY-SA 4.0</a>,
via <a href="https://commons.wikimedia.org/wiki/File:Gelnhausen_Johanniterhof_81-015.jpg" class="external-link">Wikimedia
Commons</a>
</td>
</tr>
</tbody>
</table>
<p>Photogrammetry is the art, science, and technology of obtaining
spatial information about physical objects and environments through
processes of capturing, measuring and interpreting photographic 2d
images <a href="references">(Salma, 1980)</a>by mean of <a href="https://en.wikipedia.org/wiki/Triangulation_(computer_vision)" class="external-link"><strong>triangulation
in computer vision</strong></a>. This process uses different algorithms
such as the <a href="https://www.cs.jhu.edu/~misha/ReadingSeminar/Papers/Triggs00.pdf" class="external-link"><strong>Bundle
Adjustment</strong></a> <em>“which refines simultaneously 3d coordinates
describing the geometry of the scene, the parameters of the relative
motion, and the optical characteristics of the camera(s) employed to
acquire the images, given a set of images depicting a number of 3D
points from different viewpoints”</em></p>
<table class="table">
<colgroup>
<col width="50%">
<col width="50%">
</colgroup>
<tbody>
<tr class="odd">
<td><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/2c/TriangulationIdeal.svg/1280px-TriangulationIdeal.svg.png?20070822205517" alt="Triangulation Ideal" class="figure"></td>
<td><img src="https://upload.wikimedia.org/wikipedia/commons/c/c2/Fotogrammetria_digitale.jpg" alt="Comparison between analogue and digital" class="figure"></td>
</tr>
<tr class="even">
<td>Triangulation Ideal, Public domain, under <a href="https://creativecommons.org/licenses/by-sa/4.0" class="external-link">CC BY-SA 4.0</a>,
via <a href="https://commons.wikimedia.org/wiki/File:TriangulationIdeal.svg" class="external-link">Wikimedia
Commons</a>
</td>
<td>Comparison between analogue and digital stereometric cameras -
DaddabboA, Public domain, under <a href="https://creativecommons.org/licenses/by-sa/4.0" class="external-link">CC BY-SA 4.0</a>,
via <a href="https://commons.wikimedia.org/wiki/File:Confronto_analogic0_digital.jpg" class="external-link">Wikimedia
Commons</a>
</td>
</tr>
</tbody>
</table>
<p><br><br></p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/fbRFU3eKGoM?si=AM60aNOivlbLvv1F" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen>
</iframe>
<p><br><br></p>
<p>In reality the reconstruction of the 3d scene is often referred as <a href="https://www.mathworks.com/help/vision/ug/structure-from-motion.html" class="external-link"><strong>SfM(Structure
from Motion)</strong></a>. For simplicity pourposes in this workshop we
are going to use the therm “photogrammetry” to describe the whole
process for capturing and analyzing the images and reconstructing the 3d
object or environment.</p>
<table class="table">
<colgroup><col width="100%"></colgroup>
<tbody>
<tr class="odd">
<td><img src="https://upload.wikimedia.org/wikipedia/commons/4/42/Sede_da_Fazenda_do_Pinhal_%28159%29%2C_N.ELAC.jpg" style="width:100.0%" alt="Sede da Fazenda do Pinhal" class="figure"></td>
</tr>
<tr class="even">
<td>Sede da Fazenda do Pinhal (159), NELAC under <a href="https://creativecommons.org/licenses/by-sa/4.0" class="external-link">CC BY-SA 4.0</a>,
via <a href="https://en.wikipedia.org/wiki/File:Sede_da_Fazenda_do_Pinhal_(163),_N.ELAC.jpg" class="external-link">Wikimedia
Commons</a>
</td>
</tr>
</tbody>
</table>
<!--| Sede da Fazenda do Pinhal (159) &copy; NELAC under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0), via [Wikimedia Commons 
| Sede da Fazenda do Pinhal (159) &copy; NELAC under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0), via [Wikimedia Commons |--><blockquote>
<p>In order to record 3d data with the use of photogrammetry there are
typical step to follow, which we will explore below, however a more
extensive workflow and more guidelines can be found on the <a href="https://archaeologydataservice.ac.uk/help-guidance/guides-to-good-practice/data-collection-and-fieldwork/close-range-photogrammetry/data-collection-and-documentation/typical-steps-for-a-crp-project/" class="external-link"><strong>ADS
website</strong></a> and also at the <a href="https://www.geodetic.com/basics-of-photogrammetry/" class="external-link"><strong>Geodeditic
System Website</strong></a>.</p>
</blockquote>
</section><section id="contextualization-where-and-when"><h2 class="section-heading">Contextualization (Where and When)<a class="anchor" aria-label="anchor" href="#contextualization-where-and-when"></a>
</h2>
<hr class="half-width">
<blockquote>
<p>As we saw in the previous chapter There are multiple methods for the
acquisition of 3d data, one of them is photogrammetry. Within this
method there are different scenarios for its usage, later on in this
workshop we will concentrate on large, medium and small objects for
Cultural Heritage.</p>
</blockquote>
<p><br><br></p>
<div class="section level3">
<h3 id="photogrammetry-techniques">
<strong>Photogrammetry techniques</strong><a class="anchor" aria-label="anchor" href="#photogrammetry-techniques"></a>
</h3>
<p>There are noumerous photogrammetry’s applications. Different areas of
science and humanities are using this methods to produce maps and 3d
models of the real world. It is however up to the experts in each field
of research, survey and production to choose and apply the most
appropriate technique.</p>
<p>The three main techniques are:</p>
<ul>
<li><strong>Space photogrammetry</strong></li>
<li><strong>Arial born photogrammetry</strong></li>
<li><strong>Terrestrial photogrammetry (long, medium and close
range)</strong></li>
</ul>
<p>The disciplines where these techniques are applied are:</p>
<p><br></p>
<ul>
<li><strong>Geo-science</strong></li>
</ul>
<p>Photogrammetry in geoscience is mostly used to produce <a href="https://archaeologydataservice.ac.uk/help-guidance/guides-to-good-practice/data-analysis-and-visualisation/gis/creating-and-using-gis-datasets/digital-elevation-models/" class="external-link"><strong>DEM
(Digital elevetion models)</strong></a>, however it can also be used to
study local terrain properties or part of a land by capturing the 3d
models of rocks, soil-type and vegetation, as well as studying their
conditions on a timescale. An example of this type of study can be found
in <a href="https://esurf.copernicus.org/preprints/esurf-2018-53/esurf-2018-53-manuscript-version3.pdf" class="external-link"><strong>this
article</strong></a> and <a href="https://pdf.sciencedirectassets.com/271791/1-s2.0-S0169555X12X00230/1-s2.0-S0169555X12004217/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHkaCXVzLWVhc3QtMSJHMEUCIF1YiilvObpxmTvHHmDoVJ75Y79snr6l9aUM9yZzNrM3AiEA7F5WaVfnWT9q5%2FgggFVSNJ1FijRmW2uWnqLNTXZ3NcUqswUIERAFGgwwNTkwMDM1NDY4NjUiDGKp4KNKUlQ8WrOLJCqQBaz68o4Y8LiFDt2CFA8Ly5D%2FyqQTAneRz9gXe0dI4n6VUynZLJDpjp3ix1vbtLZ3WW9dzi1NSNoBU7%2Fn5gRbew4N%2BS%2FrED6kvBr%2F7Oqls0f8GfPGAw06WAPmieYqS0Fl1A9jTJf7YAC1Wn02CnbfX7eo%2FqP6AHLie7qdameUp3dzNdzf3zeNEaxxTuiD%2Fwxo1SW5M8yCiKNToPLH%2FqIIcE7HiTY2%2BszDBB3YMdo%2BOTKyJetc9HAdVI8bqi7UpFu8o7cc9ES%2BOyngPAoXXQKOuV6k2WHBNtoxQlukcYD1kl54vpsXrA6908xR8JK%2FstXGT50oCVF2MkK0dQyPqlv5hAmqbrPQDn6DX90zuZHYxflK9LfUst29AWE6T8RLDEerTar6092bR27Elm6r7f%2BhnoJ2frTMbeP0VF0GMWxgCiBTkvDXZAIm2ByZCh4BB4f2%2F207HX%2FOjXheodEDhDSs%2Fix6UugjkvABXxxoiXhOFkKuxTXdUzG0xpjyDDGti4pHSYHbGfxwDILOq8ncUNXgvi185PmrQXJCRMUOVEc3pxVob%2BSzQug3xGm4GkgyhWBOGyd8jPtk6FOU1X80G8%2Fbq%2FpVvUTyIjuoFNUw47hHwGIVQMi1ear1QpRenpB2Eghv3l5z8Ep2YhIm4nhZeLjPa6hF6loSMMRabrVF2mJbRbIAX2auoHrpiv04e4tFrROmr%2BrndAZl%2FJDyZ03qK39ezIvNPtgCz9uO7gZRg37CfQ3NM7ek4pt4zYznZ%2BFanz%2FSC%2BccKGQ2qM1ERMSwqETMJiy5QzSIeFgMSzaYDP5D%2BpY9K3H%2BRkk2oD%2Bkb7Y5bYOdUIrA6Ibb4gbvm0ONxZCr2bqW2TFJzBONPhX1epTW4xfdMJfHxKwGOrEBB%2F4d2cZjr5d%2FvGS2wcYSuUDSwNTejfiDsoQjBY1%2FzcOKfIBhHXcYk%2FDio50wHQTkCdMFjB4pI9i%2BV8W%2Bb897Fu8x%2BXm%2BPLjEcrYqCrIMM54Va%2FaWDF7ofaOqT9ddG06VefelpHwhPveFIHniHxA7dNRW8o0M0NAxbn%2BtM2rLpUJQ3kpMmQ20hH20CFfp30Xh0IhP3HQv46JTE9zvqICWD5AEFMxTvwT3HckGCb79IgTL&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20231231T095114Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=300&amp;X-Amz-Credential=ASIAQ3PHCVTYYYB7TB6F%2F20231231%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=192fb9b454734f766543082407c5dab1274c24ba35226c064e0703d5b4e3d77a&amp;hash=40a582c434931007f377d9a7976913e561eaa82ff849cbe20208126291083e51&amp;host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&amp;pii=S0169555X12004217&amp;tid=spdf-8eac46f4-41fe-4b5c-8e36-c57e93843a42&amp;sid=73fcd6aa6c07564ac318d5a13d9eee95bb8bgxrqb&amp;type=client&amp;tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&amp;ua=13125b5c065e5907525103&amp;rr=83e19f71de9c3750&amp;cc=it" class="external-link"><strong>this
article</strong></a></p>
<table class="table">
<colgroup>
<col width="50%">
<col width="50%">
</colgroup>
<tbody>
<tr class="odd">
<td><img src="https://upload.wikimedia.org/wikipedia/commons/f/f2/The_difference_between_Digital_Surface_Model_%28DSM%29_and_Digital_Terrain_Models_%28DTM%29_when_talking_about_Digital_Elevation_models_%28DEM%29.svg" alt="The difference between Digital Surface Model (DSM) and Digital Terrain Models (DTM) when talking about Digital Elevation models (DEM)" class="figure"></td>
<td><img src="https://upload.wikimedia.org/wikipedia/commons/1/1b/Digital_Elevation_Model_-_Red_Rocks_Amphitheater%2C_Colorado.jpg" style="width:100.0%" alt="Digital Elevation Model - Red Rocks Amphitheater, Colorado" class="figure"></td>
</tr>
<tr class="even">
<td>The difference between Digital Surface Model (DSM) and Digital
Terrain Models (DTM) when talking about Digital Elevation models (DEM),
Public domain, Arbeck, under <a href="https://creativecommons.org/licenses/by-sa/4.0" class="external-link">CC BY-SA 4.0</a>,
via <a href="https://commons.wikimedia.org/wiki/File:The_difference_between_Digital_Surface_Model_(DSM)_and_Digital_Terrain_Models_(DTM)_when_talking_about_Digital_Elevation_models_(DEM).svg" class="external-link">Wikimedia
Commons</a>
</td>
<td>Digital Elevation Model - Red Rocks Amphitheater, Colorado, Public
Domain, Stoermerjp, under <a href="https://creativecommons.org/licenses/by-sa/3.0" class="external-link">CC BY-SA 3.0</a>,
via <a href="https://commons.wikimedia.org/wiki/File:Digital_Elevation_Model_-_Red_Rocks_Amphitheater,_Colorado.jpg" class="external-link">Wikimedia
Commons</a>
</td>
</tr>
</tbody>
</table>
<p><br></p>
<ul>
<li><strong>Archaeology and Cultural Heritage</strong></li>
</ul>
<p>Photogrammetry in Archeology and Cultural Heritage is used for many
different purposes. It can be used every time a recording of a three
dimensional shape or surface is required for scientific research,
archiving and publication.</p>
<blockquote>
<p>Although we have listed this as a sperate discipline, archaeology
itself can be considered multidisciplinary. Therefore photogrammetry in
this area has many different uses. <a href="https://www.mdpi.com/2071-1050/13/9/5319" class="external-link"><strong>This
paper</strong></a> and <a href="https://historicengland.org.uk/images-books/publications/photogrammetric-applications-for-cultural-heritage/heag066-photogrammetric-applications-cultural-heritage/" class="external-link"><strong>this
guideline</strong></a> from <strong>Historic England</strong> can help
you to understand this tool under this domain. <a href="https://www.researchgate.net/profile/Predrag-Novakovic-2/publication/322096576_3D_Digital_Recording_of_Archaeological_Architectural_and_Artistic_Heritage/links/5a44bce80f7e9ba868a7d110/3D-Digital-Recording-of-Archaeological-Architectural-and-Artistic-Heritage.pdf?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoicHVibGljYXRpb24iLCJwcmV2aW91c1BhZ2UiOiJfZGlyZWN0In19" class="external-link"><strong>Here</strong></a>
yo can find also a very good guide that explains the process and the
tools for 3d recording of Archaeological, Architectural and Artistic
Heritage.</p>
</blockquote>
<p>We can distinguish two main categories of photogrammetry in this
field, however the second category can be further divided into more
specific ones. In this workshop we will focus mainly on the second
category:</p>
<ul>
<li><p>Long range Arial photography (and phptogrammetry), which we
already introduced above in geo-science.</p></li>
<li>
<p>Close-range photogrammetry</p>
<ul>
<li><p>Long-distance, which includes 3d capturing of
<strong>terrains</strong>(to create DTMs), this can be performed also on
a terrestrial level when the area of study is not too large, otherwise
it will be preferable to use an Arial-born techniques such as the use of
a drone or other elevated point of capturing.</p></li>
<li><p>Medium-distance, which includes <strong>excavations</strong> and
<strong>buildings</strong> This type of photogrammetry</p></li>
</ul>
</li>
</ul>
<table class="table">
<colgroup>
<col width="50%">
<col width="50%">
</colgroup>
<tbody>
<tr class="odd">
<td><iframe width="560" height="315" title="Escavações Fórum Ammaia" frameborder="0" allowfullscreen mozallowfullscreen="true" webkitallowfullscreen="true" allow="autoplay; fullscreen; xr-spatial-tracking" xr-spatial-tracking execution-while-out-of-viewport execution-while-not-rendered web-share src="https://sketchfab.com/models/c5d03b04499f44ccb43a67193b471d14/embed">
</iframe></td>
<td></td>
</tr>
<tr class="even">
<td>Escavações Fórum Ammaia, Fundacao Cidade De Ammaia, under <a href="http://www.ammaia.pt/" class="external-link">Fundacao Cidade De Ammaia</a>, via <a href="https://sketchfab.com/3d-models/escavacoes-forum-ammaia-c5d03b04499f44ccb43a67193b471d14" class="external-link">SketchFab</a>
</td>
<td></td>
</tr>
<tr class="odd">
<td><img src="https://upload.wikimedia.org/wikipedia/commons/5/56/Saint-Sulpice_Pointcloud_colored_coords.png" alt="Saint-Sulpice Pointcloud colored coords" class="figure"></td>
<td></td>
</tr>
<tr class="even">
<td>Saint-Sulpice Pointcloud colored coords, Richard Vock, under <a href="https://creativecommons.org/licenses/by-sa/4.0" class="external-link">CC BY-SA 4.0</a>,
via <a href="https://commons.wikimedia.org/wiki/File:Saint-Sulpice_Pointcloud_colored_coords.png" class="external-link">Wikimedia
Commons</a>
</td>
<td></td>
</tr>
</tbody>
</table>
<p><br></p>
<ul>
<li>Short-distance which includes large and medium objects such as
<strong>piece of structures, pottery, sculptures, bones and other
artefacts</strong>
</li>
</ul>
<table class="table">
<colgroup>
<col width="50%">
<col width="50%">
</colgroup>
<tbody>
<tr class="odd">
<td><img src="https://upload.wikimedia.org/wikipedia/commons/5/5c/Sfm1.jpg" style="width:85.0%" alt="Sfm1" class="figure"></td>
<td><img src="https://upload.wikimedia.org/wikipedia/commons/e/e9/Wall_Painting.png" alt="Wall Painting" class="figure"></td>
</tr>
<tr class="even">
<td>Sfm1,Public Domain, Maiteng, under <a href="https://creativecommons.org/licenses/by-sa/4.0" class="external-link">CC BY-SA 4.0</a>,
via <a href="https://commons.wikimedia.org/wiki/File:Sfm1.jpg" class="external-link">Wikimedia
Commons</a>
</td>
<td>Wall Painting Photogrammetry Scan and made in 3dmax, Public Domain,
DanielNaseri, under <a href="https://creativecommons.org/licenses/by-sa/4.0" class="external-link">CC BY-SA 4.0</a>,
via <a href="https://commons.wikimedia.org/wiki/File:Wall_Painting.png" class="external-link">Wikimedia
Commons</a>
</td>
</tr>
</tbody>
</table>
<table class="table">
<colgroup>
<col width="33%">
<col width="33%">
<col width="33%">
</colgroup>
<tbody>
<tr class="odd">
<td><img src="https://data.d4science.org/shub/E_TGhlVFBIeFhkcWtzcE5KVFRVaXYyelJRdUk3YmtGRFByK2x5ZnhlUE9OK05LZU5NanUrSUtnSWptbVJLaWt6Ug==" style="width:75.0%" alt="Spring and Summer" class="figure"></td>
<td><img src="https://data.d4science.org/shub/E_dXdkL1VLYnFySzJ6d0puZVN0aDBCQktnUzY0TlNzdGNCM3FHZVg5enBkV1d3M1NDaUZ5dHJQY0NINU1MS0pWaA==" style="width:85.0%" alt="Spring and Summer" class="figure"></td>
<td><img src="https://data.d4science.org/shub/E_SG1ONWlMVFB3TW1YQkRZcXBVeDNXUlpzV1ZIYU9uWEY4OG5EblZnenNmRXdYeVY1SlBSQTdJZlNicFVzTXVTbA==" style="width:85.0%" alt="Spring and Summer" class="figure"></td>
</tr>
<tr class="even">
<td>
<strong>Photo</strong> of <a href="https://www.publicsculpturesofsussex.co.uk/object?id=184" class="external-link">“Spring
and Summer”</a> statues at Preston Park Brighton, DSVMC University of
Brighton, under <a href="https://culturedigitalskills.org/" class="external-link">DSVMC</a>,
via <a href="https://services.d4science.org/" class="external-link">D4Science</a>
</td>
<td>
<strong>Mesh</strong> of <a href="https://www.publicsculpturesofsussex.co.uk/object?id=184" class="external-link">“Spring
and Summer”</a> statues at Preston Park Brighton, DSVMC University of
Brighton, under <a href="https://culturedigitalskills.org/" class="external-link">DSVMC</a>,
via <a href="https://services.d4science.org/" class="external-link">D4Science</a>
</td>
<td>
<strong>Final Model</strong> <a href="https://www.publicsculpturesofsussex.co.uk/object?id=184" class="external-link">“Spring
and Summer”</a> statues at Preston Park Brighton, DSVMC University of
Brighton, under <a href="https://culturedigitalskills.org/" class="external-link">DSVMC</a>,
via <a href="https://services.d4science.org/" class="external-link">D4Science</a>
</td>
</tr>
</tbody>
</table>
<table class="table">
<colgroup><col width="100%"></colgroup>
<tbody>
<tr class="odd">
<td><iframe width="560" height="315" title="AMM_0029" frameborder="0" allowfullscreen mozallowfullscreen="true" webkitallowfullscreen="true" allow="autoplay; fullscreen; xr-spatial-tracking" xr-spatial-tracking execution-while-out-of-viewport execution-while-not-rendered web-share src="https://sketchfab.com/models/3f26545258c74732b4fd84c79d3a924b/embed">
</iframe></td>
</tr>
<tr class="even">
<td>AMM_0029, Fundacao Cidade De Ammaia, under <a href="http://www.ammaia.pt/" class="external-link">Fundacao Cidade De Ammaia</a>, via <a href="https://sketchfab.com/3d-models/amm-0029-3f26545258c74732b4fd84c79d3a924b" class="external-link">SketchFab</a>
</td>
</tr>
</tbody>
</table>
<p><br></p>
<ul>
<li>Closeup-distance, which includes <strong>all small objects, pottery,
osteological or lithic pieces</strong>.</li>
</ul>
<table class="table">
<colgroup><col width="100%"></colgroup>
<tbody>
<tr class="odd">
<td><img src="https://upload.wikimedia.org/wikipedia/commons/9/92/Shell_in_real_%28left%29_and_Mesh_%28right%29_from_Meshroom_-_visualization.png" style="width:100.0%" alt="Shell in real (left) and Mesh (right) from Meshroom" class="figure"></td>
</tr>
<tr class="even">
<td>Visualization of a real shell and a 3D model from 120 images.,
Public Domain, Colin Kranz, under <a href="https://creativecommons.org/licenses/by-sa/4.0" class="external-link">CC BY-SA 4.0</a>,
via <a href="https://commons.wikimedia.org/wiki/File:Shell_in_real_(left)_and_Mesh_(right)_from_Meshroom_-_visualization.png" class="external-link">Wikimedia
Commons</a>
</td>
</tr>
</tbody>
</table>
<table class="table">
<colgroup><col width="100%"></colgroup>
<tbody>
<tr class="odd">
<td><iframe width="560" height="315" title="Taça Terra Sigillata." frameborder="0" allowfullscreen mozallowfullscreen="true" webkitallowfullscreen="true" allow="autoplay; fullscreen; xr-spatial-tracking" xr-spatial-tracking execution-while-out-of-viewport execution-while-not-rendered web-share src="https://sketchfab.com/models/32cf8d2e9aec4c2cbb0c71ad54b8a01b/embed">
</iframe></td>
</tr>
<tr class="even">
<td>Taça Terra Sigillata, Fundacao Cidade De Ammaia, under <a href="http://www.ammaia.pt/" class="external-link">Fundacao Cidade De Ammaia</a>, via <a href="https://sketchfab.com/3d-models/taca-terra-sigillata-32cf8d2e9aec4c2cbb0c71ad54b8a01b" class="external-link">SketchFab</a>
</td>
</tr>
</tbody>
</table>
<p><br></p>
<ul>
<li><strong>Surveying and construction</strong></li>
</ul>
<p>Photogrammetry in construction, civil survey and architecture is very
well established. Because of the large amount of data that are usually
produced in this field are heavy on the process so that a <strong>Point
Cloud</strong> in different format is used. This permits the
visualization of millions of points at the same time. All the points
have Cartesian coordinates in space and they can also contain RGB values
for color representation. In this area of application in fact the final
3d mesh it is only produced if strictly necessary while the point cloud
model is usally used as a backdrop to reconstruct the more precise CAD
model in a <a href="https://constructible.trimble.com/construction-industry/what-is-bim-building-information-modeling" class="external-link">BIM
environment</a>. The most used software to visualize point clouds before
importing them into a more specific CAD software are <a href="https://www.meshlab.net/" class="external-link"><strong>Meshalb</strong></a> and <a href="https://www.danielgm.net/cc/" class="external-link"><strong>CloudCompare</strong></a>.</p>
<table class="table">
<colgroup><col width="100%"></colgroup>
<tbody>
<tr class="odd">
<td><img src="https://upload.wikimedia.org/wikipedia/commons/4/4b/Scan-to-BIM-ava.jpg" style="width:100.0%" alt="Scan-to-BIM-ava" class="figure"></td>
</tr>
<tr class="even">
<td>Scan to BIM is process to convert pointclouds, which are generated
by Laser scanning, to BIM model, Public Domain, Quynhphamnhat, under <a href="https://creativecommons.org/licenses/by-sa/4.0" class="external-link">CC BY-SA 4.0</a>,
via <a href="https://commons.wikimedia.org/wiki/File:Scan-to-BIM-ava.jpg" class="external-link">Wikimedia
Commons</a>
</td>
</tr>
</tbody>
</table>
<iframe width="560" height="315" src="https://www.youtube.com/embed/yXCkyuo8bcs?si=5om0ytc9ZMO_UrLy" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen>
</iframe>
<blockquote>
<p>Along with the video above <a href="https://www.dronegenuity.com/point-clouds/" class="external-link"><strong>this
page</strong></a> explains more in depth the origin and the use of a
point cloud.</p>
</blockquote>
<p><br></p>
<ul>
<li><strong>Film and Entertainment</strong></li>
</ul>
<p>In the recent years, with the advent of powerful graphic cards and
new graphic engines, photogrammetry has started to be implemented in the
film and video games industry. Rather then manually build and texturing
real environment and characters, 3d companies have now realized that
capturing models directly from the real worlds has its own advantages.
One of this tool is <a href="https://www.capturingreality.com/" class="external-link"><strong>Reality
Capture</strong></a> which is able to capture 3d objects from your
mobile phone to import them in your game developing pypeline or any
other apps.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/MjIPC5Rm6ss?si=-XdA6z8nw-_nXpYj" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen>
</iframe>
<p><br><br></p>
<ul>
<li><strong>Forensic</strong></li>
</ul>
<p>In this field photogrammetry has an extensive usage One o the tool
mainly used is <a href="https://www.photomodeler.com/forensic-photogrammetry-case-study/" class="external-link">Photomodeler</a>
who has been one of the pioneer software in this discipline.</p>
<table class="table">
<colgroup><col width="100%"></colgroup>
<tbody>
<tr class="odd">
<td><img src="https://upload.wikimedia.org/wikipedia/commons/f/fe/SfM_PPT_GUI_vs_PHOTO.png" style="width:100.0%" alt="SfM PPT GUI vs PHOTO" class="figure"></td>
</tr>
<tr class="even">
<td>Real photo x photo scan with texture color x photo scan with simple
shader. Made with Python Photogrammetry Toolbox and rendered in Blender
with Cycles, Cicero Moraes, under <a href="https://creativecommons.org/licenses/by-sa/4.0" class="external-link">CC BY-SA 4.0</a>,
via <a href="https://commons.wikimedia.org/wiki/File:SfM_PPT_GUI_vs_PHOTO.png" class="external-link">Wikimedia
Commons</a>
</td>
</tr>
</tbody>
</table>
<blockquote>
<p>You can read more about the use of photogrammetry in forensic surface
application in <a href="https://open.bu.edu/ds2/stream/?#/documents/375440/page/1" class="external-link"><strong>this
paper</strong></a></p>
</blockquote>
<blockquote>
<p>In <a href="https://www.pix4d.com/blog/five-industries-that-use-photogrammetry/" class="external-link">this
article</a> you can find 5 industries that uses photogrammetry for
different purposes.</p>
</blockquote>
<blockquote>
<p>We can conclude that on a technical level the purpose of
photogrammetry is the reconstruction (or at least the 3d visualization)
of an object or a scene in 3 dimensions. However this is not the only
technique to achieve it and there are <a href="https://archaeologydataservice.ac.uk/help-guidance/guides-to-good-practice/data-analysis-and-visualisation/3d-models/creating-3d-data/sources-and-types-of-3d-data/" class="external-link">various
sources of 3d data</a>.</p>
</blockquote>
<p><br><br></p>
<!--
---
title: 'Capturing images'
teaching: 10
exercises: 2
--- -->
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li><p>What are the premises before a shooting session?</p></li>
<li><p>How do we capture images ?</p></li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li><p>Explains what is photogrammetry</p></li>
<li><p>Shows various scenarios where this technique can be applied to
capture real world objects and environments.</p></li>
</ul>
<!-- - Advantages and disadvantages for the use of this techniques.-->
</div>
</div>
</div>
</div>
</div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li><p>What are the premises before a shooting session?</p></li>
<li><p>How do we capture images ?</p></li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Shows various scenarios of image capturing adn shooting
environments.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
</section><section id="camera-setup"><h2 class="section-heading">Camera setup<a class="anchor" aria-label="anchor" href="#camera-setup"></a>
</h2>
<hr class="half-width">
<p>Please <a href="index.html#equipment">refer to the equipment</a> that
you will need access to.</p>
<p>You will need to consider the following camera settings during your
acquisition of photographs:</p>
<ul>
<li>Preferably shoot in <strong>RAW</strong> and in maximal resolution.
JPG compression creates noise that should be avoided. If JPG images are
to be used, then prefer high quality JPG images.</li>
<li>
<strong>ISO</strong> values should be the <strong>lowest
possible</strong> as you want <strong>clear, sharp images</strong>
without too much noise. ISO 100 will provide good pictures without much
noise
<!--but for this you will need a tripod because longer shutter speeds will be required. For hand-held camera you can go up to ISO 800 but this will bring more grain to your pictures.-->
</li>
<li>
<strong>Aperture</strong> value (f number) should be <strong>high
enough</strong> so as to be able to distinguish details without having
blurred surfaces. A higher f number means that you will get a
<strong>better depth of field</strong>. Something between f/8 and f/16
would work well.</li>
<li>
<strong>Shutter speed</strong> should be <strong>fast
enough</strong> to freeze images and avoid blur that is caused by the
movement of the camera. If you are using a tripod you can use slower
shutter speeds. The rule here is that anything below <strong>(slower)
than 1/60 of a second requires a tripod</strong>.</li>
<li>You should consider always a <strong>large depth of field</strong>
when possible as <strong>good focus</strong> especially on the subject
is important. Be careful to have all the important parts of the image in
focus. Automatic focus is not advice because you would not want to
change focus for each picture you take. Once you have your subject in
focus keep it as it is and try not to change your distance from the
subject. For a better explanation on how <strong>depth of field</strong>
works in conjunction with <strong>aperture, focal length and focus
distance</strong> you can refer to the website <a href="https://www.cambridgeincolour.com/tutorials/depth-of-field.htm" class="external-link">Cambridge
in Color</a> or <a href="https://photographylife.com/what-is-depth-of-field" class="external-link">Photographylife</a><!--can be used when you are rotating around the object, but you can set focus manually if you are using a turntable-->.</li>
</ul>
<table class="table">
<colgroup>
<col width="50%">
<col width="50%">
</colgroup>
<tbody>
<tr class="odd">
<td><img src="https://upload.wikimedia.org/wikipedia/commons/6/6f/Depth-of-field.svg" alt="Depth-of-field" class="figure"></td>
<td><iframe width="560" height="315" src="https://www.youtube.com/embed/oXOHRkMHDC8?si=Ci1AADklFc2Eidjo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></td>
</tr>
<tr class="even">
<td>(a) top: large depth of field (b) bottom: small depth of field (2)
depth of field (1) plane of focus, Public domain, under <a href="https://creativecommons.org/licenses/by-sa/4.0" class="external-link">CC BY-SA 4.0</a>,
via <a href="https://upload.wikimedia.org/wikipedia/commons/6/6f/Depth-of-field.svg" class="external-link">Wikimedia
Commons</a>
</td>
<td>How to Use Depth of Field in Photography - Explained, Photography
Life, under <a href="https://photographylife.com/" class="external-link">Photography Life</a>,
via <a href="https://www.youtube.com/watch?v=oXOHRkMHDC8" class="external-link">YouTube</a>
</td>
</tr>
</tbody>
</table>
<p><br></p>
<blockquote>
<p><strong>Example of settings:</strong> f/8, ISO 400, shutter speed
1/30 and if light is not enough you can increase ISO up to 800 Or better
to lower the shutter speed to 1/15 (remember that any shutter speed that
is lower than 1/60 requires a tripod). Please note that these are just
examples and you should check exposure for every acquisition depending
on current light conditions.</p>
</blockquote>
</section><section id="capturing-images"><h2 class="section-heading">Capturing images<a class="anchor" aria-label="anchor" href="#capturing-images"></a>
</h2>
<hr class="half-width">
<p>The process for capturing photograph in photogrammetry differs
significantly from classical photography. Basic rules must be followed
in order to achieve good results. Start the acquisition from an
angle/view of the object that has many details and is not very plain. <a href="https://www.3dflow.net/technology/documents/photogrammetry-how-to-acquire-pictures/" class="external-link"><strong>Here</strong></a>
you can find an introduction for best practice when shoothing for
photogrammetry. This is from the makers of the software that we are
going to use later in the processing stage.</p>
<div class="section level3">
<h3 id="overlapping">Overlapping<a class="anchor" aria-label="anchor" href="#overlapping"></a>
</h3>
<p>You need enough image overlap, around 50-60%, to make sure that the
software will be able to align the images correctly.</p>
<p>Remember that you should avoid having <em>blind-zones</em> and the
object should occupy the maximum possible frame area.</p>
<p>Close-up photos are allowed to capture minor details.</p>
<table class="table">
<colgroup><col width="100%"></colgroup>
<tbody>
<tr class="odd">
<td><img src="https://upload.wikimedia.org/wikipedia/commons/d/d7/%EC%A7%80%EC%83%81%EC%88%98%ED%8F%89%EC%B4%AC%EC%98%81.png" style="width:100.0%" alt="Ground horizontal shooting" class="figure"></td>
</tr>
<tr class="even">
<td>Ground horizontal shooting, Gcd822C under <a href="https://creativecommons.org/licenses/by-sa/4.0" class="external-link">CC BY-SA 4.0</a>,
via <a href="https://commons.wikimedia.org/wiki/File:%EC%A7%80%EC%83%81%EC%88%98%ED%8F%89%EC%B4%AC%EC%98%81.png" class="external-link">Wikimedia
Commons</a>
</td>
</tr>
</tbody>
</table>
</div>
<div class="section level3">
<h3 id="number-of-images">Number of images<a class="anchor" aria-label="anchor" href="#number-of-images"></a>
</h3>
<p>20-60 for each 360 acquisition. Remember that it is better to have
more images than less. <em>Bad</em> images (e.g. blurred, not in focus)
can be deleted before processing.</p>
<table class="table">
<colgroup><col width="100%"></colgroup>
<tbody>
<tr class="odd">
<td><img src="https://data.d4science.org/shub/E_UTNCcDhmdUlGV21qZmdQazROUXpBTXordC9ndTV3cnJHTG9QWkJDTm5wZFVnbFlTYUJiSFhqQklGNEU0KzZySQ==" style="width:100.0%" alt="Statue of Queen Victoria Brighton" class="figure"></td>
</tr>
<tr class="even">
<td>Statue of Queen Victoria Brighton, DSVMC University of Brighton,
under <a href="https://culturedigitalskills.org/" class="external-link">DSVMC</a>, via <a href="https://www.d4science.org/" class="external-link">D4Science</a>
</td>
</tr>
</tbody>
</table>
</div>
<div class="section level3">
<h3 id="texture">Texture<a class="anchor" aria-label="anchor" href="#texture"></a>
</h3>
<p>Plain and monotonous surfaces should be avoided.</p>
<p>Flat, very thin artefacts and textures such as fur, hair won’t be the
ideal candidates for photogrammetry.</p>
<p>Crossing objects (e.g. leaves of a tree) and moving objects are not
good candidates either. Significant colour changes or colour designs on
a relatively plain surface could provide good reference points and help
us to produce a model. The best candidates are solid, matte, textured
artefacts.</p>
<table class="table">
<colgroup>
<col width="50%">
<col width="50%">
</colgroup>
<tbody>
<tr class="odd">
<td><img src="https://upload.wikimedia.org/wikipedia/commons/6/6c/LN_plain_ware_jug_-_Herakleion_AM_02.jpg" style="width:90.0%" alt="LN plain ware jug - Herakleion AM 02" class="figure"></td>
<td><img src="https://upload.wikimedia.org/wikipedia/commons/4/4c/Baekja-White_Ceramic.jpg" style="width:100.0%" alt="Baekja-White Ceramic" class="figure"></td>
</tr>
<tr class="even">
<td>
<strong>Good candidate</strong> object type / vase shape: Cretan
neolithic jug with 2 rows of smaller and larger lug handles - material:
pottery (clay) - decoration technique: undecorated, burnished - style /
ware: plain ware - period: late neolithic (dating in the museum:
3600-3000) - findspot: Ida Cave or Amnisos?? - museum / inventory
number: Heraklion, Archaeological Museum, Public domain, ArchaiOptix,
under <a href="https://creativecommons.org/licenses/by-sa/4.0" class="external-link">CC BY-SA
4.0</a>, via <a href="https://commons.wikimedia.org/wiki/File:LN_plain_ware_jug_-_Herakleion_AM_02.jpg" class="external-link">Wikimedia
Commons</a>
</td>
<td>
<strong>Bad candidate</strong> Baekja or Korean White Ceramic
(백자;白磁) in an exhibition titled “The Light of Millennium of Korean
Ceramics” in Jakarta, Indonesia, November 2008, Public Domain, Taman
Renyah, under <a href="https://creativecommons.org/licenses/by-sa/3.0" class="external-link">CC BY-SA 3.0</a>,
via <a href="https://commons.wikimedia.org/wiki/File:Baekja-White_Ceramic.jpg" class="external-link">Wikimedia
Commons</a>
</td>
</tr>
</tbody>
</table>
</div>
<div class="section level3">
<h3 id="reflections-and-transparency">Reflections and Transparency<a class="anchor" aria-label="anchor" href="#reflections-and-transparency"></a>
</h3>
<p>Some objects are shiny and the reflections will result in having lots
of noise, hence a ‘bad’ model. Transparency is also a problem because it
is very hard to estimate the 3d position of a glass as the algorithm can
get confused on the real position of the object.</p>
<p>Adding talc or corn-starch on the surface of the object could be a
solution but this cannot be applied on most cultural heritage
artefacts.</p>
<table class="table">
<colgroup><col width="100%"></colgroup>
<tbody>
<tr class="odd">
<td><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/4b/Balsamario_in_vetro_trasparente_1_13S9902.tif/lossy-page1-1280px-Balsamario_in_vetro_trasparente_1_13S9902.tif.jpg" style="width:100.0%" alt="Balsamario in vetro trasparente IEU 0370" class="figure"></td>
</tr>
<tr class="even">
<td>Transparent glass unguent bottle Glass, Egyptian Museum Turin (IT),
under <a href="https://creativecommons.org/licenses/by-sa/4.0" class="external-link">CC BY-SA
4.0</a>, via <a href="https://commons.wikimedia.org/wiki/File:Balsamario_in_vetro_trasparente_IEU_0370.tif" class="external-link">Wikimedia</a>
</td>
</tr>
</tbody>
</table>
</div>
<div class="section level3">
<h3 id="targetsmarkers">Targets/markers<a class="anchor" aria-label="anchor" href="#targetsmarkers"></a>
</h3>
<p>You can put markers and targets on/around/underneath the object that
you want to acquire to help the software with the aligning process.</p>
<p>To support accurate measurements of 3D data you can also place a
calibrated scale image underneath the object (or scale bars around
it).</p>
<p>Remember that these points should remain in the same position with
respect to the object. So, if you move with the camera around the object
they should remain in the same place (e.g. placed around the object) but
if you are using a turntable they should turn along with the object
(e.g. placed underneath the object).</p>
<table class="table">
<colgroup><col width="100%"></colgroup>
<tbody>
<tr class="odd">
<td><img src="https://data.d4science.org/shub/E_aEQwQ29YZEl4Qnc1aVE5Y0ovZHhmYWhvSHdFbTBaSGdSQ1BFUy9MUzZEbkNOUmdqKzZIMTh5ZjBaZlhjdEt0ag==" style="width:100.0%" alt="Wooden horse" class="figure"></td>
</tr>
<tr class="even">
<td>Wooden horse, DSVMC University of Brighton, under <a href="https://culturedigitalskills.org/" class="external-link">DSVMC</a>, via <a href="https://www.d4science.org/" class="external-link">D4Science</a>
</td>
</tr>
</tbody>
</table>
</div>
<div class="section level3">
<h3 id="lighting">Lighting<a class="anchor" aria-label="anchor" href="#lighting"></a>
</h3>
<p>Good lighting is required and occlusions should be kept to minimum.
The ideal conditions for an outdoor acquisition require an
overcast/cloudy day.</p>
<p>If there is sun that creates shadows, you can use a sheet to shade
the object of interest when possible.</p>
<p>For indoor acquisition, you can use static artificial light. In this
case, lights should have the same intensity. It is better to use
diffused light that is projected on every surface of the object equally.
Two light sources can be placed on the sides of the object at an angle
of 45 degrees and one can come from the top. Shadows should be avoided
as much as possible (thus you might want to add more light sources, for
example one at the back).</p>
<table class="table">
<colgroup>
<col width="50%">
<col width="50%">
</colgroup>
<tbody>
<tr class="odd">
<td><img src="https://upload.wikimedia.org/wikipedia/commons/b/b7/D%C3%BClmen%2C_St.-Viktor-Kirche_--_2014_--_0076.jpg" style="width:90.0%" alt="Saint Viktor of Xanten Church" class="figure"></td>
<td><img src="https://data.d4science.org/shub/E_UGlQR3RLSzdPZm4xZ0ZINlVXOVR1Ukd4aTFNV1V6RWxRRU16aEE1dEV6TkhYZ0wyQTBuSW0zdFN6U1d4V1NEMg==" style="width:90.0%" alt="Statue of Queen Victoria Brighton (UK)" class="figure"></td>
</tr>
<tr class="even">
<td>
<strong>Bad candidate</strong> Saint Viktor of Xanten Church,
Dülmen, North Rhine-Westphalia, Germany, Public domain, Dietmar Rabich ,
under <a href="https://creativecommons.org/licenses/by-sa/4.0" class="external-link">CC BY-SA
4.0</a>, via <a href="https://commons.wikimedia.org/wiki/File:D%C3%BClmen,_St.-Viktor-Kirche_--_2014_--_0076.jpg" class="external-link">Wikimedia
Commons</a>
</td>
<td>
<strong>Good candidate</strong> Statue of Queen Victoria Brighton,
DSVMC University of Brighton, under <a href="https://culturedigitalskills.org/" class="external-link">DSVMC</a>, via <a href="https://www.d4science.org/" class="external-link">D4Science</a>
</td>
</tr>
</tbody>
</table>
</div>
<div class="section level3">
<h3 id="background">Background<a class="anchor" aria-label="anchor" href="#background"></a>
</h3>
<p>This should be kept simple and plain. There should be high contrast
between the object and the background (e.g. dark object requires bright
background and viceversa).</p>
<table class="table">
<colgroup><col width="100%"></colgroup>
<tbody>
<tr class="odd">
<td><img src="https://upload.wikimedia.org/wikipedia/commons/d/d3/Balkan_Heritage_Field_School-5.jpg" style="width:90.0%" alt="Balkan Heritage Field School-5" class="figure"></td>
</tr>
<tr class="even">
<td>Balkan Heritage Field School (photogrammetry course) at Stobi,
Republic of Macedonia, Ivan.giogio, under <a href="https://creativecommons.org/licenses/by-sa/4.0" class="external-link">CC BY-SA 4.0</a>,
via <a href="https://commons.wikimedia.org/wiki/File:Balkan_Heritage_Field_School-5.jpg" class="external-link">Wikimedia
Commons</a>
</td>
</tr>
</tbody>
</table>
</div>
</section><section id="different-types-of-setups-indoor"><h2 class="section-heading">Different types of setups (indoor)<a class="anchor" aria-label="anchor" href="#different-types-of-setups-indoor"></a>
</h2>
<hr class="half-width">
<div class="section level3">
<h3 id="object-on-turntable-and-camera-on-tripod">Object on turntable and camera on tripod<a class="anchor" aria-label="anchor" href="#object-on-turntable-and-camera-on-tripod"></a>
</h3>
<p>The object is placed on a turntable and the camera on a tripod.</p>
<p>If artificial light is used, this should be diffused and should not
create shadows.</p>
<p>The camera should be placed at a height that allows to see all
important features of the artefact (e.g. at an angle of 45 degrees above
the object).</p>
<p>The advantage of this method is that you can have lower ISO and
shutter speeds and thus sharper images (especially in indoor
environments).</p>
<table class="table">
<colgroup>
<col width="50%">
<col width="50%">
</colgroup>
<tbody>
<tr class="odd">
<td><iframe width="560" height="315" src="https://www.youtube.com/embed/REA3XNgUMJg?si=mFPGNKpIaUf5crHb" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></td>
<td><img src="https://data.d4science.org/shub/E_dUFhdW4vd2x0SVRNOWFjOWVjc1pwU2FyaFJXWlA2VnBzWjI1QnN5L3UvalMxYlVVSFhkMmEwb0FqTlkrdHZaMg==" style="width:100.0%" alt="Wooden Horse Turntable" class="figure"></td>
</tr>
<tr class="even">
<td>Photogrammetry Setup for Indoor 3D Prop Scanning,Grzegorz Baran,
under <a href="https://www.artstation.com/gbaran" class="external-link">Grzegorz Baran</a>,
via <a href="https://www.youtube.com/watch?v=REA3XNgUMJg" class="external-link">YouTube</a>
</td>
<td>Wooden Horse Turntable, DSVMC University of Brighton, under <a href="https://culturedigitalskills.org/" class="external-link">DSVMC</a>, via <a href="https://www.d4science.org/" class="external-link">D4Science</a>
</td>
</tr>
</tbody>
</table>
<blockquote>
<p>Here you can also find a <a href="https://www.youtube.com/watch?v=Fj7wGGXPM0A" class="external-link"><strong>video</strong></a>
of an interesting automatic DIY rig that will speed up the process when
shooting small and medium objects by <a href="https://en.openscan.eu/" class="external-link"><strong>Openscan.eu</strong></a></p>
</blockquote>
<p><br></p>
</div>
<div class="section level3">
<h3 id="object-at-the-centre-and-camera-moves-around">Object at the centre and camera moves around<a class="anchor" aria-label="anchor" href="#object-at-the-centre-and-camera-moves-around"></a>
</h3>
<p>The object is placed at the centre. You move around it and take
pictures with the camera. Place the item at a good height so that it is
possible to take images from a higher and a lower level.</p>
<p>Start by taking an image every 10-15 degrees horizontally with 50-60%
overlapping.</p>
<p>As soon as you finish with one series of images around the object,
raise (or lower) the camera 10-15 degrees vertically and take another
round of photos. In case some areas are not that visible, remember to
take different pictures of that part from different angles. The
advantage of this method is that it will allow you to acquire larger
objects without setting up lights.</p>
<p>Here a <strong>video</strong> of an interesting DIY rig with camera
moving around, maintaining the same distance from the object.</p>
<table class="table">
<colgroup><col width="100%"></colgroup>
<tbody>
<tr class="odd">
<td><iframe width="560" height="315" src="https://www.youtube.com/embed/28vrZIj-hYQ?si=ue8yPd0GA0-QMgv1&amp;start=487" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></td>
</tr>
<tr class="even">
<td>Building a 3D Scanner Turntable: Advanced Photogrammetry Agisoft
Metashape, Eric Strebel, under <a href="https://www.botzen.com/" class="external-link">Eric
Strebel</a>, via <a href="https://www.youtube.com/watch?v=28vrZIj-hYQ&amp;t=487s" class="external-link">YouTube</a>
</td>
</tr>
</tbody>
</table>
<p><br></p>
</div>
<div class="section level3">
<h3 id="digital-acquisition-trough-3d-software">Digital Acquisition trough 3d software<a class="anchor" aria-label="anchor" href="#digital-acquisition-trough-3d-software"></a>
</h3>
<p>Another interesting techniques comes btween the blending of
photogrammetry and 360 footage. In the video below we can it is
explained how from 360 footgae we can extract 3d information of the
content of the footage.</p>
<table class="table">
<colgroup><col width="100%"></colgroup>
<tbody>
<tr class="odd">
<td><iframe width="560" height="315" src="https://www.youtube.com/embed/sPAC81BY_Q0?si=Ukk8fpSSBvNdlhSR" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></td>
</tr>
<tr class="even">
<td>Using 360 Video for Photogrammetry (Blender Tutorial, ShaggyMummy,
under <a href="https://www.newamsterdamphotovideo.com/" class="external-link">ShaggyMummy</a>,
via <a href="https://www.youtube.com/watch?v=sPAC81BY_Q0" class="external-link">YouTube</a>
</td>
</tr>
</tbody>
</table>
</div>
</section><section id="tips-for-acquisition"><h2 class="section-heading">Tips for acquisition<a class="anchor" aria-label="anchor" href="#tips-for-acquisition"></a>
</h2>
<hr class="half-width">
<p>Good acquisition of images is important in order to have a successful
result. Please have in mind that the right/optimal setup will provide
you with the correct dataset that will work properly with the software
and will provide you with a good 3D model.</p>
<ul>
<li>Try overalpping of at least 60% of each image</li>
<li>Avoid shooting agianst the sun when you are outside</li>
<li>Capture images with good texture.</li>
<li>Avoid completely texture-less, transparent and reflective images.
The software will have difficulty finding and matching features. If the
scene does not contain enough texture itself, you could place additional
background objects, such as posters, etc.</li>
</ul>
<p><br><br><!--
---
title: 'Processing overview'
teaching: 10
exercises: 2
--- --></p>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How do you create 3d models for 3D digital preservation and
publication ?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Shows various scenarios of image capturing adn shooting
environments.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How do you create 3d models for 3D digital preservation and
publication ?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>To explains the techniques for creating 3d objects from 2d images
positioned at a different interval in space with specialized
software.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>Multiple 2d photographs can be used to generate <a href="https://en.wikipedia.org/wiki/Point_Cloud" class="external-link">point clouds</a> where
each point has now three-dimensional coordinates.</p>
<figure><img src="https://data.d4science.org/shub/E_aFlFV0paV3RQaGQwTkJrTGVpd0pVVktJdFpEeXh4b2gySU8yMjJTNGJybFc4Z2JNS2tqWm5raHRXK0U4VHFVRA==" alt="" class="figure mx-auto d-block"><figcaption><em>Dense Point Cloud example of a small
object</em></figcaption></figure><p>These points can be further use to create a 3d meshes by mean of <a href="https://en.wikipedia.org/wiki/Mesh_generation" class="external-link">another type of
triangulation</a>.</p>
<figure><img src="https://data.d4science.org/shub/E_a0hoL2Y2dmZpREorYjNGTkx3QXBGcnZoQUd5NlhIVHQ0eStLZkVMd0hXN2RhckxRMDM5dG9ralpMaFFlSEs4cg==" alt="" class="figure mx-auto d-block"><figcaption><em>3d Mesh of a small object</em></figcaption></figure><p>The mesh can then be <a href="https://en.wikipedia.org/wiki/Texture_mapping" class="external-link">texture mapped</a>
for the final realistic appearance of the studied subject.</p>
<figure><img src="https://data.d4science.org/shub/E_ZEp0UkZxbFFvdUVXN29QMmtqWldTdDlBRnBhZUdUcTBPZUhJbG44ZEFLOEsxM2R5dlBZaE1yUG9XVUZzcHZBVg==" alt="" class="figure mx-auto d-block"><figcaption><em>3d Mesh of a small object with texture</em></figcaption></figure><iframe src="https://gltf-viewer.donmccurdy.com#kiosk=1&amp;model=https://data.d4science.org/shub/E_ZXp0WWx5S3JiVjE2RFc3WkVoMjhJSlUyUmpCWUFEQUdCSVlqamY2aC9zRUVGdWZLYWRVV0Vwem0xMHRiRkYwWQ==" style="width: 100%;" height="400px" frameborder="0">
</iframe>
<!--Underlying technology is more familiar
that we think! We can happily ignore 
the concepts and formulas used 
in the software. 

But it is useful to be aware of what it works.-->
<table class="table"><tbody></tbody></table>
<div class="section level3">
<h3 id="basic-steps-in-the-processing-phase">Basic steps in the processing phase<a class="anchor" aria-label="anchor" href="#basic-steps-in-the-processing-phase"></a>
</h3>
<p><strong>1. Feature detection</strong> (originally performed manually
but now performed automatically by the algorithm of the software)</p>
<p><strong>2. Feature matching</strong> (originally performed manually
but now performed automatically by the algorithm of the software)</p>
<p><strong>3. Structure reconstruction</strong> (performed automatically
by the algorithm of the software)</p>
</div>
<div class="section level3">
<h3 id="features-detection">Features detection<a class="anchor" aria-label="anchor" href="#features-detection"></a>
</h3>
<p>Features are “interest points” or “key points” in an image. The goal
of this step is to find points which are repeatable and distinctive.
Corners and other distinctive patterns in the image are obvious features
to consider.</p>
<div id="try-it-yourself" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="try-it-yourself" class="callout-inner">
<h3 class="callout-title">Try it yourself?<a class="anchor" aria-label="anchor" href="#try-it-yourself"></a>
</h3>
<div class="callout-content">
<p>Open this image in Gimp or others photo editing software and try to
recognize 6 or more distinctive features.</p>
<p><a href="https://data.d4science.org/shub/E_VGNNb0R2VVltRmxaOHlhSXZnczIrTkZkL1ZUUXZlTElBLzBWTHUzenREdXZSb1RMcXNwdDBNS1Qwb2d3aWNnWQ==" class="external-link">Match-1</a></p>
<p>What points would you choose?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p><a href="https://data.d4science.org/shub/E_WW9zZUluVUxmVzJFRlpDcFV3UE5MeHVJNU96d25LWlJDdDhZZlJSQnpjcWptZVowRS9YcGxHWHZUN0RmLzVlSQ==" class="external-link">Here</a>
you can download an image of the possible solution. You will need to
zoom into the image to see the exact feature points.</p>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="features-matching">Features matching<a class="anchor" aria-label="anchor" href="#features-matching"></a>
</h3>
<p>Find correspondences of features across different views. The goal of
this step is to detect (at least some of) correspondence between
features in two or more images.</p>
<div id="try-it-yourself-1" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="try-it-yourself-1" class="callout-inner">
<h3 class="callout-title">Try it yourself?<a class="anchor" aria-label="anchor" href="#try-it-yourself-1"></a>
</h3>
<div class="callout-content">
<p>Open this image in Gimp or others photo editing software and try to
recognize 6 or more features already found in the previous image.</p>
<p><a href="https://data.d4science.org/shub/E_NXBISUtZTnhDbHVGNHNxUXh0cEQzSGVldFVPMEtWWisyVU8xVmFCWWliTTNEQWIwNGx2VldUQ0xhWUZOMkk2SA==" class="external-link">Match-2</a></p>
<p>Do the features below correspond with each other?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<p><a href="https://data.d4science.org/shub/E_WFZSR0Z0Y29CTzNMNmVTdWNxelZqdFc2bkxOV3VuWU1nc0ViMVQ2MVU3RmtVMGZYd1NWclU4b24zWjB6R3VTUA==" class="external-link">Here</a>
you can download an image of the solution. You will need to zoom into
the image to see the exact feature points.</p>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="structure-reconstruction">Structure reconstruction<a class="anchor" aria-label="anchor" href="#structure-reconstruction"></a>
</h3>
<!--Load all extracted features from an 
initial pair of images. Builds a 
projection of the points in 3D space by using the camera position.-->
<p>The software will recognize the features from all the loaded images.
Builds a projection of the points in 3D space by using the camera
position.</p>
<p>The scene is incrementally extended by adding new images and
triangulating new points. A much denser set of features is produced.</p>
<p>The output of this process is a “point cloud” or a <a href="#definition">collection of points</a>. The 3D model is created by
creating a <a href="#definition">triangular mesh</a>. The texture is
then mapped to the <a href="#definition">surface</a>.</p>
<figure><img src="https://data.d4science.org/shub/E_bU9MSEZaRGpOaGFJZ2hsL1dCWi85U0NZbUJiVDh5YlBlUmxmTGI3UE9ic1dvOEdkOGFpS3JnYmRrelYrY0JOaQ==" alt="" class="figure mx-auto d-block"><figcaption><em>Matching features in multiple images</em></figcaption></figure><p>We can apply a mask to the whole sets of images so that the algorithm
does not have to calculate the points that are not interested. In this
case when using turntables is recommended to shoot always one image
without the object.</p>
<figure><img src="https://data.d4science.org/shub/E_L3Y4dlE2Rm9ZVU1BcCtSaHFoS1A5UHZpUEpXYVdaK2tRNm9MOGdjT1Y0YXE3bkdvR2FTdU1MSlp1R3ozRVVwYg==" alt="" class="figure mx-auto d-block"><figcaption><em>Mask used in multiple images</em></figcaption></figure><figure><img src="https://data.d4science.org/shub/E_dUFhdW4vd2x0SVRNOWFjOWVjc1pwU2FyaFJXWlA2VnBzWjI1QnN5L3UvalMxYlVVSFhkMmEwb0FqTlkrdHZaMg==" alt="" class="figure mx-auto d-block"><figcaption><em>Reconstructed model from matching features in
multiple images</em></figcaption></figure><p><br><br></p>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What software is available to create 3d models after image
acquisition?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>To explains the techniques for creating 3d objects from 2d images
positioned at a different interval in space with specialized
software.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What software is available to create 3d models after image
acquisition?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Explains the techniques for using specific software for processing
the images and for creating 3d models.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<!--
## Processing the images

Once the photographs have been acquired, 
the next step is to transfer the images 
to a PC (refer to [setup for specifications][setup]).


Whatever processing you apply to images 
(e.g. brightness, colour adjustments) 
should be applied to the whole dataset. 
Otherwise you will have alignment problems. 

Resizing, rotating and transforming the 
geometry of the images should not be applied.
-->
</div>
</section><section id="preparing-the-images"><h2 class="section-heading">Preparing the images<a class="anchor" aria-label="anchor" href="#preparing-the-images"></a>
</h2>
<hr class="half-width">
<p>Once the photographs have been acquired, the next step is to transfer
the images to a PC (refer to <a href="index.html#equipment">setup</a>
for specifications).</p>
<p>Please use the software we downloaded in the setup section <a href="https://www.3dflow.net" class="external-link">Raw Therapee</a>. With this free software
you can convert the images from the raw file format to various other
formats. The raw file format of different cameras is probably already
the best file format you can use during the processing of the images in
the photogrammetry software, because it retains the exif data, the most
accurate color range and the best resolution. However not all the
photogrammetry software are able to read the different type of raw
formats of the different camera types. In this case you will need to use
software such as Raw therapy (which you can also use to re-organize your
files) to convert them in a more readable format. Usually the
uncompressed Tiff or Tif file format is a good choice, because many
photogrammetry software can read it and it will retain good quality
information, including all the ones of the camera at the moment of
shooting.</p>
<p>Whatever file format you use it must be readable form the software
you are about to use for reconstructing the models and you must be sure
you choose a format that retains the Exif information within the file.
if you are not sure if your file has the information needed there are
lots of online tools that can provide such information. One example is
<a href="https://exifinfo.org/" class="external-link">ExifInfo.org</a>. (Raw Therapee can also
provide this information on the info panel however you should always
check at least one of the image after exporting them from Raw
therapy.</p>
<blockquote>
<p>For this lessons the images that you have downloaded from the <a href="setup">examples data sets</a> are already converted for you and,
although it is better for you to get use to software for batch
converting images, at this stage you will not need to take further
actions after downloading them*</p>
</blockquote>
<!--
### Deleting the background 

If you wish to delete the 
background in the images, open one of the 
images that contain a large area of the 
background cloth in the image processing
software. 

Use the colour picker (circled in red) to select 
an area of the cloth. 

Click with the right mouse button on the 
background to select that area.

Go to "save as" and save it in the 
main project folder (NOT the images folder).

## Photogrammetry Software Workflow

The following instructions are specific
to 3DF Zephir.

Go to the workflow menu and choose *New Project*
 
### Import Images 

Browse to the folder that contains your 
images and click *Select Folder*.

Click *Create camera from each file*. 
The number of cameras that appear 
in the menu will depend on how
many images you have taken. 


Save the project in the project root folder. 

### Import Masks

Select the tools menu, 
go to import and select *Import Masks*.
 
 
Select *From background* in the Method drop down box.

Select *Replacement* in the *Operation* drop down box. 

In the *Filename template* select type in the 
name that you saved earlier in paint. 

Under tolerance, select a number from 30 to 50. 
The idea is that for each point in the image, 
it will compare it to the colour in our background 
mask image at the same point. If it is the same, 
it will not be included in the 3D model. 

The tolerance number is the amount that you 
expect the colour to vary to the background image. 
The larger the number, the larger it will allow 
a deviation from the colour to be masked out. 

Click *OK*. Then select the folder that contains the image.
 

Browse to the appropriate folder and 
click *Select Folder*.

Wait for the masks to be processed. 
The time to complete will depend on the number 
of images that you have taken.
 
 
Double click each image and look for any areas that 
have been masked out that shouldn’t have been. 
In this example, it has masked out the back of 
the model. 
Use either the rectangle or the loop tool 
(circled in red) to select the area.

 
Right click and choose *Subtract Selection*. 
This will remove the selection from the mask. 
Repeat this for all images.

Save the project so far.


### Align Photos

The next step is to align the photos. For
this, go to *Workflow* in the menu
and select *Align Photos*.
 
Select the options you want, 
but make sure the *Constrain features by mask* 
is selected. Then click *OK*.
 
 
Save the project so far. 

### Build dense cloud

Go to *Workflow* in the menu
and select *Build Dense Cloud*.


Choose the options you want and click *OK*.

Save the progress so far. 

### Build mesh

Go to *Workflow* in the menu
and select *Build Mesh*.

Choose your preferred options. 
Make sure *Source data* has *Dense cloud*
selected. 
Then click OK.
 

This process will produce a 3D model. 
The 3D model can be exported, 
or edited within the scene. 

### Build Texture

The final, optional step is to re-project 
the texture onto the 3D surface. 
This makes the photographic quality much better.


Go to *Workflow* in the menu
and select *Build Texture*.


Choose the following options. 
They are suitable for most models. 
Then click *OK*.
 
Now you will have a 3D model with the texture. -->
</section><section id="photogrammetry-software-workflow"><h2 class="section-heading">Photogrammetry Software Workflow<a class="anchor" aria-label="anchor" href="#photogrammetry-software-workflow"></a>
</h2>
<hr class="half-width">
<div class="section level3">
<h3 id="organizing-the-workspace">Organizing the workspace<a class="anchor" aria-label="anchor" href="#organizing-the-workspace"></a>
</h3>
<p>Using a suitable name which reflects your project, transfer all
images into a folder.</p>
<p>Good practice includes:</p>
<pre><code><span>    <span class="va">ResourceIDifExistent_NameofObject_DateProcessedinFormatYY.MM.DD</span></span></code></pre>
<p>Within this folder, create another one named images. Copy the images
from the camera into the images folder.</p>
<p>The following instructions are specific to <a href="https://www.3dflow.net/3df-zephyr-photogrammetry-software/" class="external-link">3DF
Zephir</a>.</p>
<p>Go to the workflow menu and choose <strong>New Project</strong>, you
will be presented with a the <em>“New project wizard window”</em>.</p>
<p>Choose the first box <em>Sparse</em> in order to go trough the all
process manually. Click <strong>Next&gt;</strong> you will be presented
with the <em>“Photos selection page”</em> .</p>
</div>
<div class="section level3">
<h3 id="importing-images">Importing Images<a class="anchor" aria-label="anchor" href="#importing-images"></a>
</h3>
<ul>
<li>Browse to the folder that contains your images and click
<strong>Select Folder</strong> or Select a your <strong>Single
Images</strong>. Click <strong>Next&gt;</strong>
</li>
<li>you will be presented with the <em>“Camera calibration page<em>”. If
you have a separate Exif file for calibrating the camera you can add it
here, and you can also manually calibrate you camera in the </em>”Modify
Calibration page”</em> otherwise go on and click
<strong>Next&gt;</strong>
</li>
</ul>
<figure><img src="https://data.d4science.org/shub/E_WHBodVMwdUZ5TXU3Z3h3MDhrQjBIMmFybkNGRXN6aEtqMUNuUWY2QlFKVmF5V1hxbzRFaDd6Q0ZIT0NsZ1BaNQ==" alt="" class="figure mx-auto d-block"><figcaption><em>Original photo</em></figcaption></figure>
</div>
<div class="section level3">
<h3 id="importing-masks-optional">Importing Masks (optional)<a class="anchor" aria-label="anchor" href="#importing-masks-optional"></a>
</h3>
<p>In the <em>“Photos selection page”</em> there is an option to import
the mask, if selected a new option will be presented and a new tool
called <strong>Masquerade</strong> will be available before importing
the images. Within this tool (which is also available from the main
interface), it will be possible to generate a Mask to apply to all the
images. The tools is quite simple to use so that if you want to try to
apply a mask you can use a sample image provided in the sets of the
downloaded dataset as a first file.</p>
<figure><img src="https://data.d4science.org/shub/E_RFF0RkVickVCSmJCSHFtZm10MEYwNXFCN2xtY2JSVmY5MmxHVlo4a01WRUppcW9GK1Z0UzlXeHJXK1hJR3pwQQ==" alt="" class="figure mx-auto d-block"><figcaption><em>Original photo of the mask</em></figcaption></figure>
</div>
<div class="section level3">
<h3 id="aligning-photos">Aligning Photos<a class="anchor" aria-label="anchor" href="#aligning-photos"></a>
</h3>
<p>The next step is to align the photos. For this:</p>
<ul>
<li>you will be presented with the <em>“Camera orientation page”</em>.
Keep the general setting and click <strong>Next&gt;</strong>
</li>
<li>you will be presented with the <em>“Start reconstruction”</em> page.
Click <strong>Run</strong>
</li>
<li>you will be presented with the <em>“Reconstruction Successful
page”</em>. Click <strong>Finish</strong>
</li>
<li>Save the project in the same <a href="#id_%20Create%20a%20folder">folder</a> created before.</li>
</ul>
<p><em>“Once the camera orientation phase has been completed, the sparse
point cloud will appear in the workspace as well as the oriented cameras
identified by blue pyramids.”</em> Now you can familiarize with the
navigation of the 3d space and the interface. For example go to
<strong>Scene-&gt; Bounding Box-&gt; Edit Bounding box</strong> and
limit the created sparse cloud within the the bounding box.This will
speed-up the process when creating the final mesh.</p>
<figure><img src="https://data.d4science.org/shub/E_dEtEY3RCZFYyVjMxMjNrOEcvYUxyUENieHpZZUdWc0g1TEVVVUtUTFhWTTRXZ0JON21tRkVKN2ZCREVjcFRxcA==" alt="" class="figure mx-auto d-block"><figcaption><em>Sparse Point Cloud</em></figcaption></figure>
</div>
<div class="section level3">
<h3 id="build-dense-cloud-optional">Build dense cloud (optional)<a class="anchor" aria-label="anchor" href="#build-dense-cloud-optional"></a>
</h3>
<p>The next step is to create a Dense PointCloud. For this:</p>
<ul>
<li>Go to <em>Workflow</em> in the menu and select <em>Advanced-&gt;
Dense Point Cloud Generation</em>.</li>
<li>you will be presented with the <em>“Dense Point Cloud Generation
wizard”</em>. <strong>Select All Cameras</strong> and click
<strong>Next&gt;</strong>
</li>
<li>you will be presented with the <em>“Dense Point Cloud Creation”</em>
page. Leave the general settings and click
<strong>Next&gt;</strong>
</li>
<li>you will be presented with the <em>“Start Densification”</em> page.
Click <strong>Run</strong>
</li>
<li>when finished you will be presented with the <em>“Dense Point Cloud
generation successful”</em> page, click <strong>Finish</strong>
</li>
<li>Save the project in the same <a href="#id_%20Create%20a%20folder">folder</a> created before.</li>
</ul>
<figure><img src="https://data.d4science.org/shub/E_enEwZ01YYXFVdVlrL3NGbXQrWkoxM2VOYk1sQkY3VVI1L014RUV5UjJsMGwvYWZvcU85endtdXpzU3A1OUpleA==" alt="" class="figure mx-auto d-block"><figcaption><em>Dense Point Cloud</em></figcaption></figure>
</div>
<div class="section level3">
<h3 id="cleaning-the-dense-cloud-optional">Cleaning the dense cloud (optional)<a class="anchor" aria-label="anchor" href="#cleaning-the-dense-cloud-optional"></a>
</h3>
<p>Before trying to create the final mesh we should delete all the
unwanted points that where generated within the bounding box. We could
do that by using the same bounding box to restrict even more the area
where the algorithm is going to be applied for the triangulation.
However in order to be accustom with the software interface, we will
delete all the unecessary points manually.</p>
<ul>
<li>Go to the <em>Editing panel</em> on your right and choose <strong>By
Hand</strong>. Choose <strong>Poly</strong> and
<strong>Remove</strong>.</li>
<li>Start selecting the points that you do not need and once selected
deleted them with the del key.</li>
<li>Once happy save the project in the same <a href="#id_%20Create%20a%20folder">folder</a> created before.</li>
</ul>
</div>
<div class="section level3">
<h3 id="building-the-mesh">Building the mesh<a class="anchor" aria-label="anchor" href="#building-the-mesh"></a>
</h3>
<p>The next step is to create a Dense PointCloud. For this:</p>
<ul>
<li>Go to <em>Workflow</em> in the menu and select <em>Advanced-&gt;
Mesh Extraction</em>
</li>
<li>you will be presented with the <em>“Mesh Generation wizard”</em>.
<strong>Drop Down</strong> the name of your dense point cloud,
<strong>Select All Cameras</strong> and click
<strong>Next&gt;</strong>
</li>
<li>you will be presented with the <em>“Surface Reconstruction”</em>
page. Leave the general settings and click
<strong>Next&gt;</strong>
</li>
<li>you will be presented with the <em>“Start Mesh Creation”</em> page.
Click <strong>Run</strong>
</li>
<li>when finished you will be presented with the <em>“Mesh Creation
successful”</em> page, click <strong>Finish</strong>. This process will
produce a 3D model.</li>
<li>Once happy save the project in the same <a href="#id_%20Create%20a%20folder">folder</a> created before.</li>
</ul>
<figure><img src="https://data.d4science.org/shub/E_eEIvTkZMYWdoM3pySDdjZUtjU0J1NGFya29vMDVyQ1ErQ0k5eHh1TVlXZkcxaVBUL21ydGlEa1NPeXU0UUhzZQ==" alt="" class="figure mx-auto d-block"><figcaption><em>High Resolution Mesh</em></figcaption></figure>
</div>
<div class="section level3">
<h3 id="building-the-texture">Building the Texture<a class="anchor" aria-label="anchor" href="#building-the-texture"></a>
</h3>
<p>The final step is to re-project the texture onto the 3D surface.For
this:</p>
<ul>
<li>Go to <em>Workflow</em> in the menu and select <em>Textured Mesh
Generation</em>
</li>
<li>you will be presented with the <em>“Textured Mesh Generation
wizard”</em>. <strong>Drop Down</strong> the name of your mesh,
<strong>Select All Cameras</strong> and click
<strong>Next&gt;</strong>
</li>
<li>you will be presented with the <em>“Texturing”</em> page. Leave the
general settings and click <strong>Next&gt;</strong>
</li>
<li>you will be presented with the <em>“Textured Mesh Generation
wizard”</em> page. Click <strong>Run</strong>
</li>
<li>when finished you will be presented with the <em>“Textured Mesh
Generation wizard result”</em> page, click <strong>Finish</strong>. Now
you will have a 3D model with the texture.</li>
<li>Save the project in the same <a href="#id_%20Create%20a%20folder">folder</a> created before.</li>
</ul>
<figure><img src="https://data.d4science.org/shub/E_R21uT3hnMGRjeGZ0WHVZUkgwTW9FLzFTYzJIaWYvVGY4RWltQ1ZkZmRiZTlndExkMEtPdGRsQll3N0UzZnBVbw==" alt="" class="figure mx-auto d-block"><figcaption><em>High Resolution Mesh Texture</em></figcaption></figure>
</div>
<div class="section level3">
<h3 id="exporting-the-mesh-with-textures-for-high-res-visualization">Exporting the mesh with textures for High-Res visualization<a class="anchor" aria-label="anchor" href="#exporting-the-mesh-with-textures-for-high-res-visualization"></a>
</h3>
<p>At this point we need to export an high resolution mesh for different
purposes. For this:</p>
<ul>
<li>Go to <em>Export</em> in the menu and select <em>Export Textured
Mesh</em>. <strong>Drop Down</strong> the name of your mesh,
<strong>Drop Down</strong> your preferred format and click
<strong>Export</strong>
</li>
<li>create another folder called <em>“Exports”</em> within the same
folder of the images and save the model in this folder.</li>
</ul>
<p><img src="https://data.d4science.org/shub/E_Zk92OG5TUEN5ZGx1ais1WS80UWdVVEZORGRDKzl1YjNLR2syMWZYY3JFcTBBVGhQTSs5MjdFZTI4NVR3U2p1ZQ==" alt="https://data.d4science.org/shub/E_Zk92OG5TUEN5ZGx1ais1WS80UWdVVEZORGRDKzl1YjNLR2syMWZYY3JFcTBBVGhQTSs5MjdFZTI4NVR3U2p1ZQ==" class="figure"><em>High
Resolution Mesh Textured</em></p>
</div>
<div class="section level3">
<h3 id="exporting-the-mesh-with-textures-for-online-publishing">Exporting the mesh with textures for online publishing<a class="anchor" aria-label="anchor" href="#exporting-the-mesh-with-textures-for-online-publishing"></a>
</h3>
<p>At this point we need to export the model at a lower resolution mesh
for online publishing. For this:</p>
<ul>
<li>Select your textured mesh in the right window <em>“Textured
Meshes”</em> <strong>Right Click</strong> on it and select
<strong>Clone</strong>. A copy of your mesh will be created.</li>
<li>Go to <em>Tools</em> in the menu and select <em>Mesh Filters-&gt;
Decimatiom</em>. You will be presented with the <em>“Mesh
decimation”</em> small window. <strong>Drop Down</strong> the name of
your second mesh, select <em>preserve boundaries</em> and <em>Apply
Filter</em>
</li>
</ul>
<p>At this point we need to regenerate the texture for the lower
resolution mesh. To do so we need to repeat the process above:</p>
<ul>
<li>Go to <em>Workflow</em> in the menu and select <em>Textured Mesh
Generation</em>
</li>
<li>you will be presented with the <em>“Textured Mesh Generation
wizard”</em>. <strong>Drop Down</strong> the name of your new mesh,
<strong>Select All Cameras</strong> and click
<strong>Next&gt;</strong>
</li>
<li>you will be presented with the <em>“Texturing”</em> page. Leave the
general settings and click <strong>Next&gt;</strong>
</li>
<li>you will be presented with the <em>“Textured Mesh Generation
wizard”</em> page. Click <strong>Run</strong>
</li>
<li>when finished you will be presented with the <em>“Textured Mesh
Generation wizard result”</em> page, click <strong>Finish</strong>. Now
you will have the new low resolution 3D model with the texture.</li>
<li>Save the project in the same <a href="#id_%20Create%20a%20folder">folder</a> created before.</li>
<li>Go to <em>Export</em> in the menu and select <em>Export Textured
Mesh</em>. <strong>Drop Down</strong> the name of your second mesh,
<strong>Drop Down</strong> the format <strong><em>.glb</em></strong> or
<strong><em>.gltf</em></strong> and click <strong>Export</strong>
</li>
<li>create another folder called <em>“Exports”</em> within the same
folder of the images and save the model in this folder.</li>
</ul>
<iframe src="https://gltf-viewer.donmccurdy.com#kiosk=1&amp;model=https://data.d4science.org/shub/E_azJzMVp6MENORnRUd0FEdElCa3g5WVBIdEQ5cldBUlJwOHkyYjRITHpTYmVUcFdIUDc1VzRhWTFGdWc5SytNVA==" style="width: 100%;" height="400px" bgcolor="#dbdbdb" frameborder="0">
</iframe>
<p><br><br></p>
</div>
<div class="section level3">
<h3 id="adding-real-world-scale">Adding real world scale<a class="anchor" aria-label="anchor" href="#adding-real-world-scale"></a>
</h3>
<p><br><br></p>
</div>
<div class="section level3">
<h3 id="final-remarks">Final Remarks<a class="anchor" aria-label="anchor" href="#final-remarks"></a>
</h3>
<p>During the whole process you will encounter more options and setting
then the ones described above. You can find a more complete and
technical advice in <a href="https://www.3dflow.net/zephyr-doc/en/Extractingadensepointcloud.html" class="external-link">this
document</a>.</p>
<p>Or if you prefer it as a PDF file you can find it <a href="https://3df-eu.fra1.digitaloceanspaces.com/zephyr-doc/3DF%20Zephyr%20Manual%207.500%20English.pdf" class="external-link">here</a></p>
<p>As well as a series of video tutorials on <a href="https://www.3dflow.net/technology/documents/3df-zephyr-tutorials/" class="external-link">this
page</a>.</p>
<p>You can also find advice and specific topic help in the official <a href="https://www.3dflow.net/forums/" class="external-link">forum</a> of the software.</p>
<p>Finally you can also join the <a href="https://discord.com/invite/3HMUKff" class="external-link">Discord Channel</a> if you
prefer.</p>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</div>
</section></section><section id="aio-360Panorama"><p>Content from <a href="360Panorama.html">360 Panorama</a></p>
<hr>
<p>Last updated on 2024-01-16 |
        
        <a href="https://github.com/culturedigitalskills/2023-digitisation-photogrammetry/edit/main/episodes/360Panorama.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li><p>What is 360 Panorama Capturing?</p></li>
<li><p>Where and when can we use 360 Panorama ?</p></li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li><p>Explains what is 360 Panorama Capturing</p></li>
<li><p>Shows various scenarios where this technique can be applied to
capture 360 environments.</p></li>
<li><p>Showa how to process and stitch 360 photo shooting.</p></li>
<li><p>Shows How to publish 360 mono panorama</p></li>
</ul>
<!-- - Advantages and disadvantages for the use of this techniques.-->
</div>
</div>
</div>
</div>
</div>
<section id="definition-what"><h2 class="section-heading">Definition (What)<a class="anchor" aria-label="anchor" href="#definition-what"></a>
</h2>
<hr class="half-width"></section><section id="capturing-images"><h2 class="section-heading">Capturing Images<a class="anchor" aria-label="anchor" href="#capturing-images"></a>
</h2>
<hr class="half-width"></section><section id="processing-and-stitching"><h2 class="section-heading">Processing and Stitching<a class="anchor" aria-label="anchor" href="#processing-and-stitching"></a>
</h2>
<hr class="half-width"></section><section id="publishing"><h2 class="section-heading">Publishing<a class="anchor" aria-label="anchor" href="#publishing"></a>
</h2>
<hr class="half-width">
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 --></section></section>
</div>
    </main>
</div>
<!-- END  : inst/pkgdown/templates/content-extra.html -->

      </div>
<!--/div.row-->
      		<footer class="row footer mx-md-3"><hr>
<div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>
        
        <a href="https://github.com/culturedigitalskills/2023-digitisation-photogrammetry/edit/main/README.md" class="external-link">Edit on GitHub</a>
        
	
        | <a href="https://github.com/culturedigitalskills/2023-digitisation-photogrammetry/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/culturedigitalskills/2023-digitisation-photogrammetry/" class="external-link">Source</a></p>
				<p><a href="https://github.com/culturedigitalskills/2023-digitisation-photogrammetry/blob/main/CITATION" class="external-link">Cite</a> | <a href="mailto:K.Rodriguez@brighton.ac.uk">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">
        
        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>
        
        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.2" class="external-link">sandpaper (0.16.2)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.3" class="external-link">pegboard (0.7.3)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.1" class="external-link">varnish (1.0.1)</a></p>
			</div>
		</footer>
</div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://culturedigitalskills.github.io/2023-digitisation-photogrammetry/aio.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "photogrammetry, digitisation, data, lesson, The Carpentries",
  "name": "All in One View",
  "creativeWorkStatus": "active",
  "url": "https://culturedigitalskills.github.io/2023-digitisation-photogrammetry/aio.html",
  "identifier": "https://culturedigitalskills.github.io/2023-digitisation-photogrammetry/aio.html",
  "dateCreated": "2023-11-29",
  "dateModified": "2024-01-16",
  "datePublished": "2024-01-16"
}

  </script><script>
		feather.replace();
	</script><!-- Matomo
    2022-11-07: we have gotten a notification that we have an overage for our
    tracking and I'm pretty sure this has to do with Workbench usage.
    Considering that I am not _currently_ using this tracking because I do not
    yet know how to access the data, I am turning this off for now.
  <script>
    var _paq = window._paq = window._paq || [];
    /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
    _paq.push(["setDocumentTitle", document.domain + "/" + document.title]);
    _paq.push(["setDomains", ["*.preview.carpentries.org","*.datacarpentry.github.io","*.datacarpentry.org","*.librarycarpentry.github.io","*.librarycarpentry.org","*.swcarpentry.github.io", "*.carpentries.github.io"]]);
    _paq.push(["setDoNotTrack", true]);
    _paq.push(["disableCookies"]);
    _paq.push(['trackPageView']);
    _paq.push(['enableLinkTracking']);
    (function() {
          var u="https://carpentries.matomo.cloud/";
          _paq.push(['setTrackerUrl', u+'matomo.php']);
          _paq.push(['setSiteId', '1']);
          var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
          g.async=true; g.src='https://cdn.matomo.cloud/carpentries.matomo.cloud/matomo.js'; s.parentNode.insertBefore(g,s);
        })();
  </script>
  End Matomo Code -->
</body>
</html><!-- END:   inst/pkgdown/templates/layout.html-->

